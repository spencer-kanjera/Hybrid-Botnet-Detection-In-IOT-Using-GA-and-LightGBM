{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Botnet Detection using GA and LightGBM\n",
    "\n",
    "\n",
    "1. Import required packages\n",
    "2. Load and prepare datasets\n",
    "3. Preprocess data\n",
    "   - Remove duplicates\n",
    "   - Handle missing values\n",
    "   - Handle infinite values\n",
    "   - Drop single-value columns\n",
    "4. Feature engineering and selection\n",
    "   - Apply SMOTE for class balancing\n",
    "   - Scale features\n",
    "   - Use GA for feature selection\n",
    "5. Train LightGBM model\n",
    "   - K-fold cross validation\n",
    "   - Performance evaluation\n",
    "6. Save results and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the needed packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "from joblib import dump\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Training data: (2934817, 19)\n",
      "Testing data: (733705, 19)\n",
      "Combined data: (3668522, 19)\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv(r\"C:\\VS code projects\\data_files\\UNSW_2018_IoT_Botnet_Final_10_best_Testing.csv\")\n",
    "data_train = pd.read_csv(r\"C:\\VS code projects\\data_files\\UNSW_2018_IoT_Botnet_Final_10_best_Training.csv\")\n",
    "\n",
    "# Concatenate datasets\n",
    "df = pd.concat([data_train, data_test], axis=0, ignore_index=True)\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Training data: {data_train.shape}\")\n",
    "print(f\"Testing data: {data_test.shape}\")\n",
    "print(f\"Combined data: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removal: Done\n",
      "Missing values removed: 0\n",
      "Column names cleaned\n",
      "Infinite values handled: 0\n",
      "Dropped 0 single-value columns\n"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "# 1. Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Duplicates removal: Done\")\n",
    "\n",
    "# 2. Handle missing values\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "missing_values_before = df[numeric_cols].isnull().sum().sum()\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "# Handle non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=np.number).columns\n",
    "for col in non_numeric_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "missing_values_after = df[numeric_cols].isnull().sum().sum()\n",
    "print(f\"Missing values removed: {missing_values_before - missing_values_after}\")\n",
    "\n",
    "# 3. Clean column names\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "print(\"Column names cleaned\")\n",
    "\n",
    "# 4. Handle infinite values\n",
    "infinite_values_before = df.isin([np.inf, -np.inf]).sum().sum()\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "infinite_values_after = df.isin([np.inf, -np.inf]).sum().sum()\n",
    "print(f\"Infinite values handled: {infinite_values_before - infinite_values_after}\")\n",
    "\n",
    "# 5. Drop single-value columns\n",
    "cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "print(f\"Dropped {len(cols_to_drop)} single-value columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows between train and test: 0\n",
      "No overlap between train and test sets.\n",
      "Feature correlations with target (attack):\n",
      "N_IN_Conn_P_DstIP    0.051966\n",
      "N_IN_Conn_P_SrcIP    0.028058\n",
      "seq                  0.016681\n",
      "max                  0.015012\n",
      "mean                 0.012895\n",
      "stddev               0.011811\n",
      "min                  0.005380\n",
      "state_number         0.002169\n",
      "drate               -0.001579\n",
      "pkSeqID             -0.018452\n",
      "srate               -0.091030\n",
      "dtype: float64\n",
      "No features are highly correlated with the target.\n",
      "Class distribution in 'attack' after SMOTE:\n",
      "attack\n",
      "1    2934448\n",
      "0    2934448\n",
      "Name: count, dtype: int64\n",
      "Number of unique classes in 'attack' after SMOTE: 2\n",
      "Shape after 10% resample: (1760668, 11), (1760668,)\n",
      "Data preparation completed:\n",
      "Training set shape: (1760668, 11)\n",
      "Test set shape: (733705, 11)\n"
     ]
    }
   ],
   "source": [
    "#SMOTE and Feature Preparation\n",
    "# Remove unnecessary columns\n",
    "# Modified feature preparation code\n",
    "columns_to_drop = [\n",
    "    # these are non numeric or categorical columns that are not useful for the model\n",
    "    'category', 'subcategory', 'proto', 'saddr', 'sport', 'daddr', 'dport', 'attack',\n",
    "    \n",
    "    # Time-related columns (often not relevant for pattern detection)\n",
    "    'starttime', 'ltime',\n",
    "    \n",
    "    # Session identifiers (unique per connection)\n",
    "    'sid', 'sessionid',\n",
    "    \n",
    "    # Redundant or derived features\n",
    "    'tcprtt', 'synack', 'ackdat',  # Often correlated with other timing features\n",
    "    'state',  # Protocol state often captured by other features\n",
    "    \n",
    "    # Location or routing specific\n",
    "    'service', 'smac', 'dmac',  # MAC addresses aren't relevant for botnet detection\n",
    "    \n",
    "    # Highly sparse or constant features\n",
    "    'trans_depth', 'response_body_len'  # Often sparse in botnet traffic\n",
    "]\n",
    "\n",
    "X = df.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "y = df['attack']\n",
    "\n",
    "# Split data before SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_with_label = X_train.copy()\n",
    "train_with_label['attack'] = y_train\n",
    "test_with_label = X_test.copy()\n",
    "test_with_label['attack'] = y_test\n",
    "\n",
    "# Check for duplicate rows between train and test\n",
    "duplicates = pd.merge(train_with_label, test_with_label, how='inner')\n",
    "print(f\"Number of duplicate rows between train and test: {len(duplicates)}\")\n",
    "if len(duplicates) == 0:\n",
    "    print(\"No overlap between train and test sets.\")\n",
    "else:\n",
    "    print(\"Warning: There is overlap between train and test sets!\")\n",
    "\n",
    "# ...after train_test_split, before SMOTE...\n",
    "\n",
    "# Check correlation of each feature with the target\n",
    "correlations = X_train.corrwith(y_train)\n",
    "print(\"Feature correlations with target (attack):\")\n",
    "print(correlations.sort_values(ascending=False))\n",
    "\n",
    "# Optionally, flag features with high correlation (e.g., > 0.95)\n",
    "high_corr = correlations[correlations.abs() > 0.95]\n",
    "if not high_corr.empty:\n",
    "    print(\"Warning: The following features are highly correlated with the target and may cause leakage:\")\n",
    "    print(high_corr)\n",
    "else:\n",
    "    print(\"No features are highly correlated with the target.\")\n",
    "# ...existing code...\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "# Show class distribution after SMOTE\n",
    "print(\"Class distribution in 'attack' after SMOTE:\")\n",
    "print(y_train_resampled.value_counts())\n",
    "print(\"Number of unique classes in 'attack' after SMOTE:\", y_train_resampled.nunique())\n",
    "\n",
    "# Resample 10% of the SMOTE data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_resampled_10, _, y_train_resampled_10, _ = train_test_split(\n",
    "    X_train_resampled, y_train_resampled, test_size=0.7, random_state=42, stratify=y_train_resampled\n",
    ")\n",
    "print(f\"Shape after 10% resample: {X_train_resampled_10.shape}, {y_train_resampled_10.shape}\")\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled_10)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Use the 10% resampled labels\n",
    "y_train_resampled = y_train_resampled_10\n",
    "\n",
    "print(\"Data preparation completed:\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to drop due to high correlation: ['max']\n",
      "Selected columns after dropping highly correlated ones: ['pkSeqID', 'seq', 'stddev', 'N_IN_Conn_P_SrcIP', 'min', 'state_number', 'mean', 'N_IN_Conn_P_DstIP', 'drate', 'srate']\n"
     ]
    }
   ],
   "source": [
    "# Convert scaled data back to DataFrame for correlation analysis\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "\n",
    "# Calculate Pearson correlation matrix\n",
    "corr_matrix = X_train_scaled_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print(f\"Features to drop due to high correlation: {to_drop}\")\n",
    "\n",
    "# Drop highly correlated features\n",
    "X_train_scaled_df.drop(columns=to_drop, inplace=True)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns).drop(columns=to_drop, errors='ignore')\n",
    "\n",
    "# Update numpy arrays for GA and model training\n",
    "X_train_scaled = X_train_scaled_df.values\n",
    "X_test_scaled = X_test_scaled_df.values\n",
    "\n",
    "# Update feature names for GA and later use\n",
    "feature_names = X_train_scaled_df.columns\n",
    "\n",
    "# List selected columns\n",
    "selected_columns = list(feature_names)\n",
    "print(f\"Selected columns after dropping highly correlated ones: {selected_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA components initialized\n"
     ]
    }
   ],
   "source": [
    "#Genetic Algorithm Feature Selection\n",
    "# Clear existing DEAP creators\n",
    "if 'FitnessMax' in creator.__dict__:\n",
    "    del creator.FitnessMax\n",
    "if 'Individual' in creator.__dict__:\n",
    "    del creator.Individual\n",
    "\n",
    "# Initialize GA components\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Setup toolbox\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, \n",
    "                 toolbox.attr_bool, n=X_train_scaled.shape[1])\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "\n",
    "# Define fitness function\n",
    "def evaluate_individual(individual, X, y, k=3, min_features=5):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) < min_features:\n",
    "        return 0.0,\n",
    "    \n",
    "    X_selected = X[:, selected_features]\n",
    "    y_array = np.array(y)  # <-- Add this line\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_selected):\n",
    "        X_train_fold = X_selected[train_idx]\n",
    "        X_val_fold = X_selected[val_idx]\n",
    "        y_train_fold = y_array[train_idx]   # <-- Use y_array\n",
    "        y_val_fold = y_array[val_idx]       # <-- Use y_array\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**lgb_params)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    \n",
    "    feature_penalty = 0.0001 * len(selected_features)/X.shape[1]\n",
    "    return np.mean(scores) - feature_penalty,\n",
    "# ...existing code...\n",
    "\n",
    "# Register GA operators\n",
    "toolbox.register(\"evaluate\", evaluate_individual, X=X_train_scaled, y=y_train_resampled)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "print(\"GA components initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA components initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "# Initialize GA components\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Setup toolbox\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, \n",
    "                 toolbox.attr_bool, n=X_train_scaled.shape[1])\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define LightGBM parameters\n",
    "lgb_params = {\n",
    "    'learning_rate': 0.01990086265614642,\n",
    "    'num_leaves': 65,\n",
    "    'max_depth': 7,\n",
    "    'min_child_samples': 40,\n",
    "    'subsample': 0.6971404971190901,\n",
    "    'colsample_bytree': 0.8912957047621014,\n",
    "    'reg_alpha': 1.1077475757773147e-05,\n",
    "    'reg_lambda': 7.505137036788418,\n",
    "    'n_estimators': 386,\n",
    "    'feature_fraction': 0.6749863286377266,\n",
    "    'bagging_fraction': 0.8742278952008886,\n",
    "    'min_child_weight': 13,\n",
    "    'random_state': 42,\n",
    "    'verbosity': -1\n",
    "    }\n",
    "\n",
    "# Define fitness function\n",
    "def evaluate_individual(individual, X, y, k=3):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0.0,\n",
    "    \n",
    "    X_selected = X[:, selected_features]\n",
    "    y_array = np.array(y)  # Always convert y to numpy array\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_selected):\n",
    "        X_train_fold = X_selected[train_idx]\n",
    "        X_val_fold = X_selected[val_idx]\n",
    "        y_train_fold = y_array[train_idx]\n",
    "        y_val_fold = y_array[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**lgb_params)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    \n",
    "    feature_penalty = 0.001 * len(selected_features)/X.shape[1]\n",
    "    return np.mean(scores) - feature_penalty,\n",
    "\n",
    "# Register GA operators\n",
    "toolbox.register(\"evaluate\", evaluate_individual, X=X_train_scaled, y=y_train_resampled)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "print(\"GA components initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Genetic Algorithm for feature selection...\n",
      "gen\tnevals\n",
      "0  \t20    \n",
      "1  \t16    \n",
      "2  \t16    \n",
      "3  \t20    \n",
      "4  \t17    \n",
      "5  \t18    \n",
      "6  \t16    \n",
      "7  \t16    \n",
      "8  \t16    \n",
      "9  \t18    \n",
      "10 \t18    \n",
      "\n",
      "Number of selected features: 2\n",
      "\n",
      "Selected Features:\n",
      "1. pkSeqID\n",
      "2. drate\n",
      "\n",
      "Selected features saved to models_and_data/selected_features.txt\n"
     ]
    }
   ],
   "source": [
    "#Run GA and Store Selected Features\n",
    "print(\"Running Genetic Algorithm for feature selection...\")\n",
    "\n",
    "# Set GA parameters\n",
    "population_size = 20\n",
    "n_generations = 10\n",
    "crossover_prob = 0.8\n",
    "mutation_prob = 0.1\n",
    "\n",
    "# Create initial population\n",
    "population = toolbox.population(n=population_size)\n",
    "\n",
    "# Run GA\n",
    "result, logbook = algorithms.eaSimple(population, toolbox,\n",
    "                                      cxpb=crossover_prob,\n",
    "                                      mutpb=mutation_prob,\n",
    "                                      ngen=n_generations,\n",
    "                                      verbose=True)\n",
    "\n",
    "# Get best solution\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "selected_features = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "chosen_features = list(feature_names[selected_features])\n",
    "\n",
    "print(f\"\\nNumber of selected features: {len(chosen_features)}\")\n",
    "print(\"\\nSelected Features:\")\n",
    "for i, feature in enumerate(chosen_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "# Save selected features\n",
    "feature_file = \"models_and_data/selected_features.txt\"\n",
    "with open(feature_file, \"w\") as file:\n",
    "    file.write(\"Selected Features from Genetic Algorithm\\n\")\n",
    "    file.write(f\"\\nTotal Features Selected: {len(chosen_features)} out of {len(feature_names)}\\n\")\n",
    "    file.write(\"\\nFeature List:\\n\")\n",
    "    for i, feature in enumerate(chosen_features, 1):\n",
    "        file.write(f\"{i}. {feature}\\n\")\n",
    "\n",
    "print(f\"\\nSelected features saved to {feature_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM with 5 k-fold validation...\n",
      "\n",
      "Training fold 1/5...\n",
      "Fold 1 Results:\n",
      "Accuracy: 0.9990\n",
      "Precision: 0.9990\n",
      "Recall: 0.9990\n",
      "F1 Score: 0.9990\n",
      "\n",
      "Training fold 2/5...\n",
      "Fold 2 Results:\n",
      "Accuracy: 0.9989\n",
      "Precision: 0.9989\n",
      "Recall: 0.9989\n",
      "F1 Score: 0.9989\n",
      "\n",
      "Training fold 3/5...\n",
      "Fold 3 Results:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Training fold 4/5...\n",
      "Fold 4 Results:\n",
      "Accuracy: 0.9990\n",
      "Precision: 0.9990\n",
      "Recall: 0.9990\n",
      "F1 Score: 0.9990\n",
      "\n",
      "Training fold 5/5...\n",
      "Fold 5 Results:\n",
      "Accuracy: 0.9990\n",
      "Precision: 0.9990\n",
      "Recall: 0.9990\n",
      "F1 Score: 0.9990\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Accuracy: 0.9992\n",
      "Precision: 0.9992\n",
      "Recall: 0.9992\n",
      "F1: 0.9992\n"
     ]
    }
   ],
   "source": [
    "#LightGBM Training and Evaluation\n",
    "print(\"Training LightGBM with 5 k-fold validation...\")\n",
    "\n",
    "# Prepare data with selected features\n",
    "X_selected = X_train_scaled[:, selected_features]\n",
    "X_test_selected = X_test_scaled[:, selected_features]\n",
    "\n",
    "# Initialize metrics storage\n",
    "fold_results = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ...existing code...\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_selected), 1):\n",
    "    print(f\"\\nTraining fold {fold}/5...\")\n",
    "\n",
    "    # Split data\n",
    "    X_train_fold = X_selected[train_idx]\n",
    "    X_val_fold = X_selected[val_idx]\n",
    "    y_train_fold = y_train_resampled.iloc[train_idx]  # <-- Use .iloc here\n",
    "    y_val_fold = y_train_resampled.iloc[val_idx]      # <-- And here\n",
    "\n",
    "    # Train model\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    metrics = {\n",
    "        \"Fold\": fold,\n",
    "        \"Accuracy\": accuracy_score(y_val_fold, y_pred),\n",
    "        \"Precision\": precision_score(y_val_fold, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_val_fold, y_pred, average='weighted'),\n",
    "        \"F1\": f1_score(y_val_fold, y_pred, average='weighted'),\n",
    "        \"Confusion_Matrix\": confusion_matrix(y_val_fold, y_pred)\n",
    "    }\n",
    "    fold_results.append(metrics)\n",
    "\n",
    "    print(f\"Fold {fold} Results:\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['F1']:.4f}\")\n",
    "# ...existing code...\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_metrics = {\n",
    "    \"Accuracy\": np.mean([m[\"Accuracy\"] for m in fold_results]),\n",
    "    \"Precision\": np.mean([m[\"Precision\"] for m in fold_results]),\n",
    "    \"Recall\": np.mean([m[\"Recall\"] for m in fold_results]),\n",
    "    \"F1\": np.mean([m[\"F1\"] for m in fold_results])\n",
    "}\n",
    "\n",
    "print(\"\\nAverage Metrics Across All Folds:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to models_and_data/final_results.txt\n",
      "Model saved to models_and_data/final_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save Results and Model\n",
    "# Train final model on all training data\n",
    "final_model = lgb.LGBMClassifier(**lgb_params)\n",
    "final_model.fit(X_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = final_model.predict(X_test_selected)\n",
    "test_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Precision\": precision_score(y_test, y_pred_test, average='weighted'),\n",
    "    \"Recall\": recall_score(y_test, y_pred_test, average='weighted'),\n",
    "    \"F1\": f1_score(y_test, y_pred_test, average='weighted'),\n",
    "    \"Confusion_Matrix\": confusion_matrix(y_test, y_pred_test)\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results_file = \"models_and_data/final_results.txt\"\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"GA-LightGBM Results\\n\\n\")\n",
    "    \n",
    "    file.write(\"Selected Features:\\n\")\n",
    "    for i, feature in enumerate(chosen_features, 1):\n",
    "        file.write(f\"{i}. {feature}\\n\")\n",
    "    \n",
    "    file.write(\"\\nCross-Validation Results:\\n\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        file.write(f\"Average {metric}: {value:.4f}\\n\")\n",
    "    \n",
    "    file.write(\"\\nTest Set Results:\\n\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        if metric != \"Confusion_Matrix\":\n",
    "            file.write(f\"{metric}: {value:.4f}\\n\")\n",
    "    file.write(f\"\\nConfusion Matrix:\\n{test_metrics['Confusion_Matrix']}\\n\")\n",
    "\n",
    "# Save model\n",
    "model_file = \"models_and_data/final_model.joblib\"\n",
    "dump(final_model, model_file)\n",
    "\n",
    "print(f\"\\nResults saved to {results_file}\")\n",
    "print(f\"Model saved to {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics saved to models_and_data/performance_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "output_file = \"models_and_data/performance_metrics.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Performance Metrics:\\n\")\n",
    "    file.write(f\"Average Accuracy: {avg_metrics['Accuracy']}\\n\")\n",
    "    file.write(f\"Average Precision: {avg_metrics['Precision']}\\n\")\n",
    "    file.write(f\"Average Recall: {avg_metrics['Recall']}\\n\")\n",
    "    file.write(f\"Average F1 Score: {avg_metrics['F1']}\\n\")\n",
    "    file.write(\"\\nResults for Each Fold:\\n\")\n",
    "    for result in fold_results:\n",
    "        file.write(f\"Fold {result['Fold']}:\\n\")\n",
    "        file.write(f\"  Accuracy: {result['Accuracy']}\\n\")\n",
    "        file.write(f\"  Precision: {result['Precision']}\\n\")\n",
    "        file.write(f\"  Recall: {result['Recall']}\\n\")\n",
    "        file.write(f\"  F1 Score: {result['F1']}\\n\")\n",
    "        file.write(f\"  Confusion Matrix:\\n{result['Confusion_Matrix']}\\n\\n\")\n",
    "\n",
    "print(f\"Performance metrics saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_and_data/final_model.joblib\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save the trained model\n",
    "model_file = \"models_and_data/final_model.joblib\"\n",
    "dump(final_model, model_file)\n",
    "\n",
    "print(f\"Model saved to {model_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
