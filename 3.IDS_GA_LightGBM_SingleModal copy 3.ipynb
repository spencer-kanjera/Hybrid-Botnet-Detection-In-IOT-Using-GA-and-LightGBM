{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Botnet Detection using GA and LightGBM\n",
    "\n",
    "\n",
    "1. Import required packages\n",
    "2. Load and prepare datasets\n",
    "3. Preprocess data\n",
    "   - Remove duplicates\n",
    "   - Handle missing values\n",
    "   - Handle infinite values\n",
    "   - Drop single-value columns\n",
    "4. Feature engineering and selection\n",
    "   - Apply SMOTE for class balancing\n",
    "   - Scale features\n",
    "   - Use GA for feature selection\n",
    "5. Train LightGBM model\n",
    "   - K-fold cross validation\n",
    "   - Performance evaluation\n",
    "6. Save results and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the needed packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import multiprocessing\n",
    "from deap import base, creator, tools, algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "data_train = pd.read_csv(r\"C:\\VS code projects\\data_files\\UNSW_2018_IoT_Botnet_Final_10_best_Training.csv\")\n",
    "data_test = pd.read_csv(r\"C:\\VS code projects\\data_files\\UNSW_2018_IoT_Botnet_Final_10_best_Testing.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Data Preprocessing\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "    non_numeric_cols = df.select_dtypes(exclude=np.number).columns\n",
    "    for col in non_numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    df.columns = df.columns.str.replace(' ', '')\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    " \n",
    "    return df\n",
    "\n",
    "data_train = preprocess(data_train)\n",
    "data_test = preprocess(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'DDoS': np.int64(0), 'DoS': np.int64(1), 'Normal': np.int64(2), 'Reconnaissance': np.int64(3), 'Theft': np.int64(4)}\n",
      "Shapes after feature selection and index reset:  (2934817, 11) (2934817,)\n"
     ]
    }
   ],
   "source": [
    "# 3. Feature selection\n",
    "columns_to_drop = [\n",
    "    'subcategory', 'proto', 'saddr', 'sport', 'daddr', 'dport', 'attack',\n",
    "    'starttime', 'ltime', 'sid', 'sessionid', 'tcprtt', 'synack', 'ackdat', 'state',\n",
    "    'service', 'smac', 'dmac', 'trans_depth', 'response_body_len'\n",
    "]\n",
    "X_train = data_train.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "y_train = data_train['category']\n",
    "X_test = data_test.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "y_test = data_test['category']\n",
    "\n",
    "# Ensure all features are numeric for X_train and X_test\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test = X_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# Reset indices so that X_train and y_train are aligned\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Encode the categorical target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print(\"Shapes after feature selection and index reset: \", X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after SMOTE: (7706575, 11) (7706575,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(\"Shape after SMOTE:\", X_train_resampled.shape, y_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Drop highly correlated features\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "corr_matrix = X_train_scaled_df.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "X_train_scaled_df.drop(columns=to_drop, inplace=True)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_train.columns).drop(columns=to_drop, errors='ignore')\n",
    "X_train_scaled = X_train_scaled_df.values\n",
    "X_test_scaled = X_test_scaled_df.values\n",
    "feature_names = X_train_scaled_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM parameters for multiclass\n",
    "lgb_params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": len(le.classes_),\n",
    "    \"random_state\": 42,\n",
    "    \"n_estimators\": 100,\n",
    "    \"n_jobs\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Genetic Algorithm for Feature Selection\n",
    "import multiprocessing\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# --- Clean up previous DEAP creators (especially in Jupyter) ---\n",
    "for name in [\"FitnessMax\", \"Individual\"]:\n",
    "    if hasattr(creator, name):\n",
    "        delattr(creator, name)\n",
    "\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "num_features = X_train_scaled.shape[1]\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=num_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# --- GA parameters ---\n",
    "population_size = 40\n",
    "n_generations = 15\n",
    "crossover_prob = 0.7\n",
    "mutation_prob = 0.15\n",
    "mutation_indpb = 0.08\n",
    "\n",
    "min_features = max(5, int(0.1 * num_features))\n",
    "penalty_factor = 0.001\n",
    "\n",
    "# --- Global cache to store fitness evaluations ---\n",
    "fitness_cache = {}\n",
    "\n",
    "def evaluate_individual(individual, X, y, min_features, n_splits=3):\n",
    "    key = tuple(individual)\n",
    "    if key in fitness_cache:\n",
    "        return fitness_cache[key]\n",
    "    selected = [i for i, bit in enumerate(individual) if bit]\n",
    "    if len(selected) < min_features:\n",
    "        fitness_cache[key] = (0.0,)\n",
    "        return (0.0,)\n",
    "    X_sel = X[:, selected]\n",
    "    y_arr = np.array(y)\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in kf.split(X_sel):\n",
    "        X_tr, X_val = X_sel[train_idx], X_sel[val_idx]\n",
    "        y_tr, y_val = y_arr[train_idx], y_arr[val_idx]\n",
    "        model = lgb.LGBMClassifier(**lgb_params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            early_stopping_rounds=10,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, y_pred))\n",
    "    mean_score = np.mean(scores)\n",
    "    penalty = penalty_factor * (len(selected) / X.shape[1])\n",
    "    fitness = mean_score - penalty\n",
    "    fitness_cache[key] = (fitness,)\n",
    "    return (fitness,)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate_individual, X=X_train_scaled, y=y_train, min_features=min_features, n_splits=3)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=mutation_indpb)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# --- Parallelize evaluation with multiprocessing ---\n",
    "with multiprocessing.Pool() as pool:\n",
    "    toolbox.register(\"map\", pool.map)\n",
    "    population = toolbox.population(n=population_size)\n",
    "    result, logbook = algorithms.eaSimple(\n",
    "        population, toolbox,\n",
    "        cxpb=crossover_prob,\n",
    "        mutpb=mutation_prob,\n",
    "        ngen=n_generations,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "# --- Plot evolution: Fitness over generations ---\n",
    "generations = logbook.select(\"gen\")\n",
    "max_fitness = logbook.select(\"max\")\n",
    "avg_fitness = logbook.select(\"avg\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(generations, max_fitness, label=\"Max Fitness\")\n",
    "plt.plot(generations, avg_fitness, label=\"Avg Fitness\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.title(\"Fitness Evolution Over Generations\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- Identify the best individual and the selected features ---\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "selected_feature_indices = [i for i, bit in enumerate(best_individual) if bit]\n",
    "chosen_features = list(np.array(feature_names)[selected_feature_indices])\n",
    "\n",
    "print(f\"\\nNumber of selected features: {len(chosen_features)}\")\n",
    "print(\"Selected Features:\")\n",
    "for i, feature in enumerate(chosen_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM with 5 k-fold validation...\n",
      "\n",
      "Training fold 1/5...\n",
      "Fold 1 Results:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[    72      8]\n",
      " [     6 586878]]\n",
      "\n",
      "Training fold 2/5...\n",
      "Fold 2 Results:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[    63      8]\n",
      " [    10 586883]]\n",
      "\n",
      "Training fold 3/5...\n",
      "Fold 3 Results:\n",
      "Accuracy: 0.9999\n",
      "Precision: 0.9999\n",
      "Recall: 0.9999\n",
      "F1 Score: 0.9999\n",
      "Confusion Matrix:\n",
      "[[    48     30]\n",
      " [     9 586876]]\n",
      "\n",
      "Training fold 4/5...\n",
      "Fold 4 Results:\n",
      "Accuracy: 1.0000\n",
      "Precision: 0.9999\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9999\n",
      "Confusion Matrix:\n",
      "[[    49     18]\n",
      " [    11 586885]]\n",
      "\n",
      "Training fold 5/5...\n",
      "Fold 5 Results:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[    71      3]\n",
      " [    13 586876]]\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"Training LightGBM with 5 stratified k-fold validation and early stopping...\")\n",
    "\n",
    "# Prepare data with selected features\n",
    "X_selected = X_train_scaled[:, selected_feature_indices]\n",
    "X_test_selected = X_test_scaled[:, selected_feature_indices]\n",
    "\n",
    "fold_results = []\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_selected, y_train_resampled), 1):\n",
    "    print(f\"\\nTraining fold {fold}/5...\")\n",
    "\n",
    "    X_train_fold = X_selected[train_idx]\n",
    "    X_val_fold = X_selected[val_idx]\n",
    "    y_train_fold = y_train_resampled.iloc[train_idx]\n",
    "    y_val_fold = y_train_resampled.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_val_fold, y_val_fold)],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    cm = confusion_matrix(y_val_fold, y_pred)\n",
    "    metrics = {\n",
    "        \"Fold\": fold,\n",
    "        \"Accuracy\": accuracy_score(y_val_fold, y_pred),\n",
    "        \"Precision\": precision_score(y_val_fold, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_val_fold, y_pred, average='weighted'),\n",
    "        \"F1\": f1_score(y_val_fold, y_pred, average='weighted'),\n",
    "        \"Confusion_Matrix\": cm\n",
    "    }\n",
    "    fold_results.append(metrics)\n",
    "\n",
    "    print(f\"Fold {fold} Results:\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['F1']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_metrics = {\n",
    "    \"Accuracy\": np.mean([m[\"Accuracy\"] for m in fold_results]),\n",
    "    \"Precision\": np.mean([m[\"Precision\"] for m in fold_results]),\n",
    "    \"Recall\": np.mean([m[\"Recall\"] for m in fold_results]),\n",
    "    \"F1\": np.mean([m[\"F1\"] for m in fold_results])\n",
    "}\n",
    "\n",
    "print(\"\\nAverage Metrics Across All Folds:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Optional: Plot per-fold metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot([m[\"Fold\"] for m in fold_results], [m[\"Accuracy\"] for m in fold_results], marker='o', label='Accuracy')\n",
    "plt.plot([m[\"Fold\"] for m in fold_results], [m[\"F1\"] for m in fold_results], marker='o', label='F1 Score')\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Per-Fold Metrics\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the held-out test set\n",
    "final_model = lgb.LGBMClassifier(**lgb_params)\n",
    "final_model.fit(X_selected, y_train_resampled)\n",
    "y_pred_test = final_model.predict(X_test_selected)\n",
    "test_cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_test, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_test, average='weighted'):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_test, average='weighted'):.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{test_cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the final LightGBM model to a file\n",
    "joblib.dump(final_model, \"final_lightgbm_model.joblib\")\n",
    "print(\"Final LightGBM model saved as 'final_lightgbm_model.joblib'.\")\n",
    "\n",
    "# To load the model later:\n",
    "# loaded_model = joblib.load(\"final_lightgbm_model.joblib\")\n",
    "# y_pred_loaded = loaded_model.predict(X_test_selected)\n",
    "# ...existing code..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
