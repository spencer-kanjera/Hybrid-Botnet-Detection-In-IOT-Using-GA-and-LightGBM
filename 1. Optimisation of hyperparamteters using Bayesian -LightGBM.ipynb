{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31b4d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f0fa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing Datasets Prepared!\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing datasets\n",
    "data_train = pd.read_csv(r\"C:\\VS code projects\\data_files\\UNSW_2018_IoT_Botnet_Final_10_best_Training.csv\")\n",
    "data_test = pd.read_csv(r\"C:\\VS code projects\\data_files\\UNSW_2018_IoT_Botnet_Final_10_best_Testing.csv\")\n",
    "\n",
    "# Define features (X) and target variable (y) for training and testing sets\n",
    "X_train = data_train.drop(['category', 'subcategory', 'proto', 'saddr', 'sport', 'daddr', 'dport', 'attack'], axis=1)\n",
    "y_train = data_train['attack']\n",
    "\n",
    "X_test = data_test.drop(['category', 'subcategory', 'proto', 'saddr', 'sport', 'daddr', 'dport', 'attack'], axis=1)\n",
    "y_test = data_test['attack']\n",
    "\n",
    "print(\"Training and Testing Datasets Prepared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb3bb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_train distribution:\n",
      "attack\n",
      "1    2934447\n",
      "0        370\n",
      "Name: count, dtype: int64\n",
      "Resampled y_train distribution:\n",
      "attack\n",
      "1    2934447\n",
      "0    2934447\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to balance the class distribution in the training dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display the class distribution after resampling\n",
    "print(f\"Original y_train distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Resampled y_train distribution:\\n{y_train_resampled.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8c10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna's Bayesian optimization.\n",
    "    Args:\n",
    "        trial: Optuna trial object for sampling hyperparameters.\n",
    "    Returns:\n",
    "        Accuracy score on the test dataset.\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    param_grid = {\n",
    "        'objective': 'binary',  # Binary classification\n",
    "        'metric': 'binary_logloss',  # Loss metric\n",
    "        'verbosity': -1,  # Suppress training logs\n",
    "        'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
    "        'random_state': 42,  # Ensure reproducibility\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 50)\n",
    "    }\n",
    "\n",
    "    # Train LightGBM model using hyperparameters sampled by Optuna\n",
    "    model = lgb.LGBMClassifier(**param_grid)\n",
    "    model.fit(\n",
    "        X_train_resampled,\n",
    "        y_train_resampled,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric='logloss',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]  # Early stopping\n",
    "    )\n",
    "\n",
    "    # Predict on the testing dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ff56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 21:56:33,466] A new study created in memory with name: lightgbm_training_testing\n",
      "[I 2025-05-09 21:56:50,254] Trial 0 finished with value: 1.0 and parameters: {'learning_rate': 0.016065205094981324, 'num_leaves': 56, 'max_depth': 5, 'min_child_samples': 18, 'subsample': 0.7572876334719514, 'colsample_bytree': 0.7753545078516937, 'reg_alpha': 3.1067877226474795e-07, 'reg_lambda': 6.2560353478602936e-06, 'n_estimators': 56, 'feature_fraction': 0.43604344159336933, 'bagging_fraction': 0.8617436259493026, 'min_child_weight': 47}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 21:57:22,282] Trial 1 finished with value: 1.0 and parameters: {'learning_rate': 0.023638700304826385, 'num_leaves': 55, 'max_depth': 3, 'min_child_samples': 91, 'subsample': 0.8947039858984942, 'colsample_bytree': 0.6388750859772981, 'reg_alpha': 0.00032093877013971697, 'reg_lambda': 1.3039255670389288e-06, 'n_estimators': 216, 'feature_fraction': 0.87984455494484, 'bagging_fraction': 0.420345354925196, 'min_child_weight': 24}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 21:58:57,523] Trial 2 finished with value: 1.0 and parameters: {'learning_rate': 0.024138839179401805, 'num_leaves': 77, 'max_depth': 11, 'min_child_samples': 58, 'subsample': 0.6644263938262902, 'colsample_bytree': 0.587862940321252, 'reg_alpha': 0.045415928392570123, 'reg_lambda': 2.4334534559316747e-06, 'n_estimators': 406, 'feature_fraction': 0.9864527595458507, 'bagging_fraction': 0.7728366822128867, 'min_child_weight': 25}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:00:00,677] Trial 3 finished with value: 1.0 and parameters: {'learning_rate': 0.012680053017207059, 'num_leaves': 108, 'max_depth': 5, 'min_child_samples': 13, 'subsample': 0.6975926016897042, 'colsample_bytree': 0.7523626887046971, 'reg_alpha': 0.0008415965327738743, 'reg_lambda': 4.783583340086368e-08, 'n_estimators': 339, 'feature_fraction': 0.8560720012885998, 'bagging_fraction': 0.5381416906950346, 'min_child_weight': 50}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:00:37,251] Trial 4 finished with value: 1.0 and parameters: {'learning_rate': 0.07750392811412354, 'num_leaves': 103, 'max_depth': 10, 'min_child_samples': 28, 'subsample': 0.8574319499527561, 'colsample_bytree': 0.5275807623475686, 'reg_alpha': 1.4235834908816724e-08, 'reg_lambda': 0.00935601308135987, 'n_estimators': 371, 'feature_fraction': 0.725601687302097, 'bagging_fraction': 0.9547911475376193, 'min_child_weight': 25}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:01:46,000] Trial 5 finished with value: 0.999998637054402 and parameters: {'learning_rate': 0.015653199776306723, 'num_leaves': 129, 'max_depth': 3, 'min_child_samples': 60, 'subsample': 0.9515454662422337, 'colsample_bytree': 0.9300195708958847, 'reg_alpha': 0.0003294229874685482, 'reg_lambda': 2.5060253940891966, 'n_estimators': 473, 'feature_fraction': 0.7665648863743535, 'bagging_fraction': 0.4982730170811312, 'min_child_weight': 12}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:02:37,084] Trial 6 finished with value: 1.0 and parameters: {'learning_rate': 0.015468634675637992, 'num_leaves': 140, 'max_depth': 9, 'min_child_samples': 34, 'subsample': 0.8888935666495767, 'colsample_bytree': 0.5894895169590202, 'reg_alpha': 0.2353288621368896, 'reg_lambda': 0.7576627613488037, 'n_estimators': 398, 'feature_fraction': 0.9398213258987083, 'bagging_fraction': 0.5614958445978881, 'min_child_weight': 48}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:03:08,618] Trial 7 finished with value: 1.0 and parameters: {'learning_rate': 0.05365941816367022, 'num_leaves': 121, 'max_depth': 8, 'min_child_samples': 58, 'subsample': 0.8341627992075378, 'colsample_bytree': 0.9440272219086278, 'reg_alpha': 3.869764386129657e-06, 'reg_lambda': 0.002872268971845503, 'n_estimators': 288, 'feature_fraction': 0.9609134242883817, 'bagging_fraction': 0.8543858896152481, 'min_child_weight': 48}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:03:32,205] Trial 8 finished with value: 1.0 and parameters: {'learning_rate': 0.07884053695626976, 'num_leaves': 28, 'max_depth': 7, 'min_child_samples': 85, 'subsample': 0.6345879302962545, 'colsample_bytree': 0.8224706110688631, 'reg_alpha': 2.208493815263777, 'reg_lambda': 2.6294717236121926e-08, 'n_estimators': 190, 'feature_fraction': 0.9122127982954353, 'bagging_fraction': 0.8097282765060316, 'min_child_weight': 14}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:04:35,822] Trial 9 finished with value: 1.0 and parameters: {'learning_rate': 0.017655243345685374, 'num_leaves': 45, 'max_depth': 11, 'min_child_samples': 98, 'subsample': 0.5551975199413864, 'colsample_bytree': 0.6841436105852092, 'reg_alpha': 0.012907098599461759, 'reg_lambda': 0.0019642131126531653, 'n_estimators': 480, 'feature_fraction': 0.8237414048390599, 'bagging_fraction': 0.6209216370278061, 'min_child_weight': 19}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:04:47,322] Trial 10 finished with value: 1.0 and parameters: {'learning_rate': 0.2574179660072065, 'num_leaves': 78, 'max_depth': 6, 'min_child_samples': 13, 'subsample': 0.783417909402798, 'colsample_bytree': 0.8302496305780341, 'reg_alpha': 5.208702563279336e-07, 'reg_lambda': 1.5015624328599874e-05, 'n_estimators': 51, 'feature_fraction': 0.42219008470039127, 'bagging_fraction': 0.9863698749508959, 'min_child_weight': 37}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:05:09,572] Trial 11 finished with value: 0.999998637054402 and parameters: {'learning_rate': 0.03532174398210569, 'num_leaves': 56, 'max_depth': 3, 'min_child_samples': 79, 'subsample': 0.9910839442008762, 'colsample_bytree': 0.6903745450189386, 'reg_alpha': 7.984530217544847e-06, 'reg_lambda': 6.261181031223531e-06, 'n_estimators': 152, 'feature_fraction': 0.5536091384486209, 'bagging_fraction': 0.4068935961320283, 'min_child_weight': 36}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:05:21,239] Trial 12 finished with value: 1.0 and parameters: {'learning_rate': 0.031145123591894704, 'num_leaves': 57, 'max_depth': 5, 'min_child_samples': 42, 'subsample': 0.7691194804563919, 'colsample_bytree': 0.6939702174883899, 'reg_alpha': 1.5518159113928e-08, 'reg_lambda': 6.383629604956852e-07, 'n_estimators': 53, 'feature_fraction': 0.6076520852152788, 'bagging_fraction': 0.6794970893051371, 'min_child_weight': 35}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:05:48,701] Trial 13 finished with value: 0.999998637054402 and parameters: {'learning_rate': 0.03982640489402784, 'num_leaves': 25, 'max_depth': 15, 'min_child_samples': 74, 'subsample': 0.8970269548093521, 'colsample_bytree': 0.8180553734766416, 'reg_alpha': 9.667042653848741e-06, 'reg_lambda': 9.366131506968482e-05, 'n_estimators': 169, 'feature_fraction': 0.45343761505309405, 'bagging_fraction': 0.8987597677780719, 'min_child_weight': 7}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:06:22,110] Trial 14 finished with value: 1.0 and parameters: {'learning_rate': 0.010592828547979025, 'num_leaves': 63, 'max_depth': 3, 'min_child_samples': 95, 'subsample': 0.7349918715608322, 'colsample_bytree': 0.6199855024501405, 'reg_alpha': 0.00173643262754367, 'reg_lambda': 3.179540979068626e-07, 'n_estimators': 241, 'feature_fraction': 0.6304312024417249, 'bagging_fraction': 0.7281575928240529, 'min_child_weight': 41}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-05-09 22:06:39,715] Trial 15 finished with value: 1.0 and parameters: {'learning_rate': 0.184816854775505, 'num_leaves': 41, 'max_depth': 5, 'min_child_samples': 41, 'subsample': 0.5745353380682306, 'colsample_bytree': 0.7537250390070507, 'reg_alpha': 2.809240858782301e-07, 'reg_lambda': 0.0001019652252656412, 'n_estimators': 112, 'feature_fraction': 0.49146773233697844, 'bagging_fraction': 0.413289810949818, 'min_child_weight': 30}. Best is trial 0 with value: 1.0.\n",
      "[W 2025-05-09 22:07:17,816] Trial 16 failed with parameters: {'learning_rate': 0.021672741548056693, 'num_leaves': 92, 'max_depth': 14, 'min_child_samples': 69, 'subsample': 0.8074611898733868, 'colsample_bytree': 0.5006801414260866, 'reg_alpha': 6.756454831056829e-05, 'reg_lambda': 1.9580412641084463e-07, 'n_estimators': 232, 'feature_fraction': 0.6330654663949166, 'bagging_fraction': 0.6526284532470146, 'min_child_weight': 19} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Spencer Kanjera\\AppData\\Local\\Temp\\ipykernel_30312\\1153767141.py\", line 32, in objective\n",
      "    model.fit(\n",
      "    ~~~~~~~~~^\n",
      "        X_train_resampled,\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]  # Early stopping\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 1284, in fit\n",
      "    super().fit(\n",
      "    ~~~~~~~~~~~^\n",
      "        X,\n",
      "        ^^\n",
      "    ...<12 lines>...\n",
      "        init_model=init_model,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 955, in fit\n",
      "    self._Booster = train(\n",
      "                    ~~~~~^\n",
      "        params=params,\n",
      "        ^^^^^^^^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        callbacks=callbacks,\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\engine.py\", line 307, in train\n",
      "    booster.update(fobj=fobj)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\basic.py\", line 4136, in update\n",
      "    _LIB.LGBM_BoosterUpdateOneIter(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._handle,\n",
      "        ^^^^^^^^^^^^^\n",
      "        ctypes.byref(is_finished),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-09 22:07:17,959] Trial 16 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create Optuna study to maximize accuracy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgbm_training_testing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Run for 50 trials or 1 hour\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Print the best trial results\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Train LightGBM model using hyperparameters sampled by Optuna\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam_grid)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_resampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_resampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Early stopping\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Predict on the testing dataset\u001b[39;00m\n\u001b[0;32m     41\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py:1284\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1281\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1282\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[1;32m-> 1284\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py:955\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    952\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    953\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "File \u001b[1;32mc:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    296\u001b[0m     cb(\n\u001b[0;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[0;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    305\u001b[0m     )\n\u001b[1;32m--> 307\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\basic.py:4136\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4135\u001b[0m _safe_call(\n\u001b[1;32m-> 4136\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4140\u001b[0m )\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "# Create Optuna study to maximize accuracy\n",
    "study = optuna.create_study(direction='maximize', study_name='lightgbm_training_testing')\n",
    "study.optimize(objective, n_trials=50, timeout=3600)  # Run for 50 trials or 1 hour\n",
    "\n",
    "# Print the best trial results\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value: {study.best_trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# ---- Visualizations ----\n",
    "\n",
    "# 1. Optimization History\n",
    "fig1 = vis.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "# 2. Hyperparameter Importance\n",
    "fig2 = vis.plot_param_importances(study)\n",
    "fig2.show()\n",
    "\n",
    "# 3. Slice Plot (shows impact of individual parameters)\n",
    "fig3 = vis.plot_slice(study)\n",
    "fig3.show()\n",
    "\n",
    "# 4. Parallel Coordinates Plot (shows interactions between hyperparameters)\n",
    "fig4 = vis.plot_parallel_coordinate(study)\n",
    "fig4.show()\n",
    "\n",
    "# 5. Contour Plot (visualizes relationships between two hyperparameters)\n",
    "fig5 = vis.plot_contour(study)\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to models_and_data/best_hyperparameters_lightgbm_training_testing.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the best hyperparameters and accuracy score to a file\n",
    "output_file = \"models_and_data/best_hyperparameters_lightgbm_training_testing.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Best Hyperparameters:\\n\")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "    f.write(f\"\\nBest Accuracy: {study.best_trial.value:.4f}\\n\")\n",
    "\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
