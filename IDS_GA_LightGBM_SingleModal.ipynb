{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the Code\n",
    "1. Importing all packages\n",
    "2. Reading the datasets\n",
    "3. Preprocessing the data (Duplicates , Missing , Infinite , NAN Values)\n",
    "4. Dropping columns with only one unique value\n",
    "5. Sampling the data \n",
    "6. Convert the Label values to single Modal\n",
    "7. Using SMOTE sampling to ensure the data is balanced\n",
    "8. Splitting the data into test and train\n",
    "9. Applying Pearson Corellation to filter out the important features\n",
    "9. Running GA\n",
    "10. Running LightGBM\n",
    "11. Pefrom kfold validation whilst collecting perfomance metrics of the algorithm\n",
    "12. Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the needed packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import pearsonr\n",
    "from deap import base, creator, tools, algorithms\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"C:/VS code projects/data_files/Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"C:/VS code projects/data_files/Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"C:/VS code projects/data_files/Wednesday-workingHours.pcap_ISCX.csv\",\n",
    "    \"C:/VS code projects/data_files/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
    "    \"C:/VS code projects/data_files/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
    "    \"C:/VS code projects/data_files/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "    \"C:/VS code projects/data_files/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "    \"C:/VS code projects/data_files/Friday-WorkingHours-Morning.pcap_ISCX.csv\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Read and clean datasets\n",
    "dataframes = []\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()  # Remove whitespace from column names\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all datasets into a single DataFrame\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cremoving duplicates: (2830743, 79)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 330963\n",
      "After removing duplicates: (2499780, 79)\n"
     ]
    }
   ],
   "source": [
    "#2.1 Dealing with duplicates\n",
    "print(f'Before Cremoving duplicates: {df.shape}')\n",
    "duplicates = df[df.duplicated()]\n",
    "print(f'Number of duplicates: {len(duplicates)}')\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(f'After removing duplicates: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "Flow Bytes/s    353\n",
      "dtype: int64\n",
      "Missing values after filling: 0\n"
     ]
    }
   ],
   "source": [
    "#2.2 Handling missing values both numeric and non-numeric columns\n",
    "# Identify columns with missing values\n",
    "missing_val = df.isna().sum()\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_val.loc[missing_val > 0])\n",
    "\n",
    "# Handle missing values for numeric columns (fill with mean)\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "# Handle missing values for non-numeric columns (fill with mode)\n",
    "non_numeric_cols = df.select_dtypes(exclude=['number']).columns\n",
    "for col in non_numeric_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Verify if there are still any missing values\n",
    "print(f\"Missing values after filling: {df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial missing values: 0\n",
      "Initial infinite values: 3126\n",
      "Columns with infinite values after processing (should be empty):\n",
      "Series([], dtype: int64)\n",
      "Missing values after dropping rows: 0\n"
     ]
    }
   ],
   "source": [
    "#2.3 Handling infinite values\n",
    "\n",
    "# Initial count of missing and infinite values\n",
    "print(f'Initial missing values: {df.isna().sum().sum()}')\n",
    "print(f'Initial infinite values: {df.isin([np.inf, -np.inf]).sum().sum()}')\n",
    "\n",
    "# Drop rows with infinite values\n",
    "df = df[~df.isin([np.inf, -np.inf]).any(axis=1)]\n",
    "\n",
    "# Verify that infinite values are removed\n",
    "inf_count = df.isin([np.inf, -np.inf]).sum()\n",
    "print(\"Columns with infinite values after processing (should be empty):\")\n",
    "print(inf_count[inf_count > 0])\n",
    "\n",
    "# Final missing value check\n",
    "print(f\"Missing values after dropping rows: {df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Bwd PSH Flags', 'Bwd URG Flags', 'Fwd Avg Bytes/Bulk',\n",
       "       'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk',\n",
       "       'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping columns with only one unique value\n",
    "num_unique = df.nunique()\n",
    "one_variable = num_unique[num_unique == 1]\n",
    "not_one_variable = num_unique[num_unique > 1].index\n",
    "\n",
    "dropped_cols = one_variable.index\n",
    "df = df[not_one_variable]\n",
    "\n",
    "print('Dropped columns:')\n",
    "dropped_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset:\n",
      "Label\n",
      "1    414369\n",
      "0    414369\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['Label'] = df['Label'].apply(lambda x: 1 if x == 'BENIGN' else 0)\n",
    "\n",
    "# Find the minimum class count\n",
    "#min_count = df['Label'].value_counts().min()\n",
    "\n",
    "\n",
    "# Perform undersampling to balance the dataset\n",
    "#df = df.groupby('Label', group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42, ignore_index=True))\n",
    "# Check the class distribution\n",
    "#print(df['Label'].value_counts())\n",
    "# Check the class distribution\n",
    "#df = df.sample(frac=0.05, random_state=42)\n",
    "#df['Label'].value_counts()\n",
    "\n",
    "df = df.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Ensure there are no NaN values in the dataset before applying SMOTE\n",
    "if df.isna().sum().sum() > 0:\n",
    "    print(\"Dataset contains NaN values. Filling NaN values with column means...\")\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "# Split the original dataset into features (X) and target (y)\n",
    "X = df.drop('Label', axis=1)  # Features\n",
    "y = df['Label']  # Target\n",
    "\n",
    "# Perform SMOTE sampling to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Combine the resampled features and target into a new DataFrame\n",
    "df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), \n",
    "                pd.DataFrame(y_resampled, columns=['Label'])], axis=1)\n",
    "\n",
    "# Display the value counts to verify balance\n",
    "print('Balanced dataset:')\n",
    "print(df['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Separate features and labels\n",
    "X = df.drop('Label', axis=1)\n",
    "Y = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Normalize numerical features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating feature importance using correlation...\n",
      "Features with correlation above threshold: 26\n",
      "0: Destination Port\n",
      "1: Flow Duration\n",
      "2: Fwd Packet Length Min\n",
      "3: Bwd Packet Length Max\n",
      "4: Bwd Packet Length Min\n",
      "5: Bwd Packet Length Mean\n",
      "6: Bwd Packet Length Std\n",
      "7: Flow IAT Mean\n",
      "8: Flow IAT Std\n",
      "9: Flow IAT Max\n",
      "10: Fwd IAT Total\n",
      "11: Fwd IAT Mean\n",
      "12: Fwd IAT Std\n",
      "13: Fwd IAT Max\n",
      "14: Min Packet Length\n",
      "15: Max Packet Length\n",
      "16: Packet Length Mean\n",
      "17: Packet Length Std\n",
      "18: Packet Length Variance\n",
      "19: FIN Flag Count\n",
      "20: URG Flag Count\n",
      "21: Average Packet Size\n",
      "22: Avg Bwd Segment Size\n",
      "23: Idle Mean\n",
      "24: Idle Max\n",
      "25: Idle Min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter Method: Calculate correlations between features and labels\n",
    "print(\"Calculating feature importance using correlation...\")\n",
    "correlations = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    if np.std(X_train[:, i]) == 0:  # Skip constant features\n",
    "        correlations.append(0)\n",
    "    else:\n",
    "        correlations.append(abs(pearsonr(X_train[:, i], y_train)[0]))\n",
    "correlation_threshold = 0.2  # Define a threshold to filter irrelevant features\n",
    "relevant_features = [i for i, corr in enumerate(correlations) if corr > correlation_threshold]\n",
    "print(f\"Features with correlation above threshold: {len(relevant_features)}\")\n",
    "\n",
    "# Subset the data with relevant features only\n",
    "X_train = X_train[:, relevant_features]\n",
    "X_test = X_test[:, relevant_features]\n",
    "feature_names = [df.columns[i] for i in relevant_features]\n",
    "[print(f\"{i}: {feature_names[i]}\") for i in range(len(feature_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Genetic Algorithm for feature selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "# GA Feature Selection\n",
    "print(\"Defining Genetic Algorithm for feature selection...\")\n",
    "\n",
    "# Define GA components\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X_train.shape[1])\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fitness function\n",
    "def fitness(individual, X_train, y_train):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:  # Avoid empty feature sets\n",
    "        return 0,\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    model = lgb.LGBMClassifier(random_state=42)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    accuracy = model.score(X_train_selected, y_train)  # Training accuracy\n",
    "    return accuracy,\n",
    "\n",
    "toolbox.register(\"evaluate\", fitness, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GA parameters\n",
    "population = toolbox.population(n=50)\n",
    "ngen = 40\n",
    "cxpb = 0.7\n",
    "mutpb = 0.2\n",
    "\n",
    "# Run the GA\n",
    "result_population = algorithms.eaSimple(population, toolbox, cxpb=cxpb, mutpb=mutpb, ngen=ngen, verbose=False)\n",
    "best_individual = tools.selBest(result_population[0], k=1)[0]\n",
    "selected_features = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "\n",
    "# Print the chosen features\n",
    "chosen_features = [feature_names[i] for i in selected_features]\n",
    "print(\"Selected Features:\")\n",
    "print(chosen_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing k-fold cross-validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 232177, number of negative: 231915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5883\n",
      "[LightGBM] [Info] Number of data points in the train set: 464092, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500282 -> initscore=0.001129\n",
      "[LightGBM] [Info] Start training from score 0.001129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 0.9955871199062263, Precision: 0.9955933998341946, Recall: 0.9955871199062263, F1: 0.9955871213918758\n",
      "Training on fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 231996, number of negative: 232097\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5881\n",
      "[LightGBM] [Info] Number of data points in the train set: 464093, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499891 -> initscore=-0.000435\n",
      "[LightGBM] [Info] Start training from score -0.000435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Accuracy: 0.9959318411004715, Precision: 0.9959331913920312, Recall: 0.9959318411004715, F1: 0.9959318343986221\n",
      "Training on fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 232242, number of negative: 231851\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5884\n",
      "[LightGBM] [Info] Number of data points in the train set: 464093, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500421 -> initscore=0.001685\n",
      "[LightGBM] [Info] Start training from score 0.001685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Accuracy: 0.995664652698172, Precision: 0.9956738418127152, Recall: 0.995664652698172, F1: 0.9956646613589956\n",
      "Training on fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 232075, number of negative: 232018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5878\n",
      "[LightGBM] [Info] Number of data points in the train set: 464093, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500061 -> initscore=0.000246\n",
      "[LightGBM] [Info] Start training from score 0.000246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Accuracy: 0.9958887461968747, Precision: 0.9958926635091048, Recall: 0.9958887461968747, F1: 0.9958887392273627\n",
      "Training on fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 231810, number of negative: 232283\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5898\n",
      "[LightGBM] [Info] Number of data points in the train set: 464093, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002038\n",
      "[LightGBM] [Info] Start training from score -0.002038\n",
      "Fold 5 - Accuracy: 0.9965437887315446, Precision: 0.9965480588528058, Recall: 0.9965437887315446, F1: 0.9965437589633337\n",
      "\n",
      "Results for Each Fold:\n",
      "Fold 1:\n",
      "  Accuracy: 0.9955871199062263\n",
      "  Precision: 0.9955933998341946\n",
      "  Recall: 0.9955871199062263\n",
      "  F1 Score: 0.9955871213918758\n",
      "  Confusion Matrix:\n",
      "[[57767   359]\n",
      " [  153 57745]]\n",
      "\n",
      "Fold 2:\n",
      "  Accuracy: 0.9959318411004715\n",
      "  Precision: 0.9959331913920312\n",
      "  Recall: 0.9959318411004715\n",
      "  F1 Score: 0.9959318343986221\n",
      "  Confusion Matrix:\n",
      "[[57660   284]\n",
      " [  188 57891]]\n",
      "\n",
      "Fold 3:\n",
      "  Accuracy: 0.995664652698172\n",
      "  Precision: 0.9956738418127152\n",
      "  Recall: 0.995664652698172\n",
      "  F1 Score: 0.9956646613589956\n",
      "  Confusion Matrix:\n",
      "[[57814   376]\n",
      " [  127 57706]]\n",
      "\n",
      "Fold 4:\n",
      "  Accuracy: 0.9958887461968747\n",
      "  Precision: 0.9958926635091048\n",
      "  Recall: 0.9958887461968747\n",
      "  F1 Score: 0.9958887392273627\n",
      "  Confusion Matrix:\n",
      "[[57703   320]\n",
      " [  157 57843]]\n",
      "\n",
      "Fold 5:\n",
      "  Accuracy: 0.9965437887315446\n",
      "  Precision: 0.9965480588528058\n",
      "  Recall: 0.9965437887315446\n",
      "  F1 Score: 0.9965437589633337\n",
      "  Confusion Matrix:\n",
      "[[57472   286]\n",
      " [  115 58150]]\n",
      "\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Accuracy: 0.9959232297266579\n",
      "Precision: 0.9959282310801703\n",
      "Recall: 0.9959232297266579\n",
      "F1 Score: 0.9959232230680379\n",
      "Confusion Matrix:\n",
      "[[288416   1625]\n",
      " [   740 289335]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "conf_matrices = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "print(\"Performing k-fold cross-validation...\")\n",
    "fold = 1\n",
    "fold_results = []  # To store results for each fold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    print(f\"Training on fold {fold}...\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    final_model = lgb.LGBMClassifier(random_state=42)\n",
    "    final_model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = final_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "    precision = precision_score(y_val_fold, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val_fold, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val_fold, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    # Store fold results\n",
    "    fold_results.append({\n",
    "        \"Fold\": fold,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Confusion Matrix\": conf_matrix\n",
    "    })\n",
    "    \n",
    "    print(f\"Fold {fold} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "    fold += 1\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_conf_matrix = np.sum(conf_matrices, axis=0)  # Sum confusion matrices across folds\n",
    "\n",
    "# Print results for each fold\n",
    "print(\"\\nResults for Each Fold:\")\n",
    "for result in fold_results:\n",
    "    print(f\"Fold {result['Fold']}:\")\n",
    "    print(f\"  Accuracy: {result['Accuracy']}\")\n",
    "    print(f\"  Precision: {result['Precision']}\")\n",
    "    print(f\"  Recall: {result['Recall']}\")\n",
    "    print(f\"  F1 Score: {result['F1 Score']}\")\n",
    "    print(f\"  Confusion Matrix:\\n{result['Confusion Matrix']}\\n\")\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics Across All Folds:\")\n",
    "print(f\"Accuracy: {avg_accuracy}\")\n",
    "print(f\"Precision: {avg_precision}\")\n",
    "print(f\"Recall: {avg_recall}\")\n",
    "print(f\"F1 Score: {avg_f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(avg_conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
