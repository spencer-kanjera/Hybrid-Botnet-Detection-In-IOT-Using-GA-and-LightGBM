{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Botnet Detection using GA and LightGBM\n",
    "\n",
    "\n",
    "1. Import required packages\n",
    "2. Load and prepare datasets\n",
    "3. Preprocess data\n",
    "   - Remove duplicates\n",
    "   - Handle missing values\n",
    "   - Handle infinite values\n",
    "   - Drop single-value columns\n",
    "4. Feature engineering and selection\n",
    "   - Apply SMOTE for class balancing\n",
    "   - Scale features\n",
    "   - Use GA for feature selection\n",
    "5. Train LightGBM model\n",
    "   - K-fold cross validation\n",
    "   - Performance evaluation\n",
    "6. Save results and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the needed packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "from joblib import dump\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original attack distribution:\n",
      "attack\n",
      "1    3668045\n",
      "0        477\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Encoded attack distribution:\n",
      "attack_encoded\n",
      "1    3668045\n",
      "0        477\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack classes:\n",
      "0: 0\n",
      "1: 1\n",
      "\n",
      "Dataset shapes:\n",
      "Training data: (2934817, 19)\n",
      "Testing data: (733705, 19)\n",
      "Combined data: (3668522, 20)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_test = pd.read_csv(r\"C:\\VS code projects\\data_files\\UNSW_2018_IoT_Botnet_Final_10_best_Testing.csv\")\n",
    "data_train = pd.read_csv(r\"C:\\VS code projects\\data_files\\UNSW_2018_IoT_Botnet_Final_10_best_Training.csv\")\n",
    "\n",
    "# Concatenate datasets\n",
    "df = pd.concat([data_train, data_test], axis=0, ignore_index=True)\n",
    "\n",
    "# Convert attack labels to multiclass\n",
    "label_encoder = LabelEncoder()\n",
    "print(\"Original attack distribution:\")\n",
    "print(df['attack'].value_counts())\n",
    "df['attack_encoded'] = label_encoder.fit_transform(df['attack'])\n",
    "print(\"\\nEncoded attack distribution:\")\n",
    "print(df['attack_encoded'].value_counts())\n",
    "print(\"\\nAttack classes:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Training data: {data_train.shape}\")\n",
    "print(f\"Testing data: {data_test.shape}\")\n",
    "print(f\"Combined data: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removal: Done\n",
      "Missing values removed: 0\n",
      "Column names cleaned\n",
      "Infinite values handled: 0\n",
      "Dropped 0 single-value columns\n",
      "\n",
      "Feature statistics by attack type:\n",
      "\n",
      "Attack type: 1\n",
      "Number of samples: 3668045\n",
      "Numeric feature statistics:\n",
      "          pkSeqID         seq      stddev  N_IN_Conn_P_SrcIP         min  \\\n",
      "count  3668045.00  3668045.00  3668045.00         3668045.00  3668045.00   \n",
      "mean   1834034.85   121335.04        0.89              82.55        1.02   \n",
      "std    1058893.55    75788.29        0.80              24.39        1.48   \n",
      "min          1.00        1.00        0.00               1.00        0.00   \n",
      "25%     917012.00    54900.00        0.03              69.00        0.00   \n",
      "50%    1834023.00   117786.00        0.79             100.00        0.00   \n",
      "75%    2751034.00   184940.00        1.75             100.00        2.15   \n",
      "max    3668522.00   262212.00        2.50             100.00        4.98   \n",
      "\n",
      "       state_number        mean  N_IN_Conn_P_DstIP       drate       srate  \\\n",
      "count    3668045.00  3668045.00         3668045.00  3668045.00  3668045.00   \n",
      "mean           3.13        2.23              92.46        0.44        2.18   \n",
      "std            1.19        1.52              18.15       60.27      286.39   \n",
      "min            1.00        0.00               1.00        0.00        0.00   \n",
      "25%            3.00        0.18             100.00        0.00        0.16   \n",
      "50%            4.00        2.69             100.00        0.00        0.28   \n",
      "75%            4.00        3.57             100.00        0.00        0.49   \n",
      "max           11.00        4.98             100.00    58823.53   250000.00   \n",
      "\n",
      "              max     attack  attack_encoded  \n",
      "count  3668045.00  3668045.0       3668045.0  \n",
      "mean         3.02        1.0             1.0  \n",
      "std          1.86        0.0             0.0  \n",
      "min          0.00        1.0             1.0  \n",
      "25%          0.28        1.0             1.0  \n",
      "50%          4.01        1.0             1.0  \n",
      "75%          4.29        1.0             1.0  \n",
      "max          5.00        1.0             1.0  \n",
      "\n",
      "Attack type: 0\n",
      "Number of samples: 477\n",
      "Numeric feature statistics:\n",
      "          pkSeqID       seq  stddev  N_IN_Conn_P_SrcIP     min  state_number  \\\n",
      "count      477.00    477.00  477.00             477.00  477.00        477.00   \n",
      "mean   3577123.00   8830.31    0.04              20.41    0.31          2.87   \n",
      "std        137.84   8542.01    0.19              21.98    0.95          1.28   \n",
      "min    3576885.00      1.00    0.00               1.00    0.00          1.00   \n",
      "25%    3577004.00   3683.00    0.00               6.00    0.00          2.00   \n",
      "50%    3577123.00   4280.00    0.00              14.00    0.01          2.00   \n",
      "75%    3577242.00  14518.00    0.00              20.00    0.17          4.00   \n",
      "max    3577361.00  33724.00    1.89              74.00    4.95          8.00   \n",
      "\n",
      "         mean  N_IN_Conn_P_DstIP    drate       srate     max  attack  \\\n",
      "count  477.00             477.00   477.00      477.00  477.00   477.0   \n",
      "mean     0.50               8.32    11.03     5943.70    0.54     0.0   \n",
      "std      1.33               8.30   133.64    58124.56    1.40     0.0   \n",
      "min      0.00               1.00     0.00        0.00    0.00     0.0   \n",
      "25%      0.00               2.00     0.00        0.00    0.00     0.0   \n",
      "50%      0.01               4.00     0.00        0.00    0.01     0.0   \n",
      "75%      0.18              14.00     0.00        0.40    0.21     0.0   \n",
      "max      4.97              85.00  2178.65  1000000.00    5.00     0.0   \n",
      "\n",
      "       attack_encoded  \n",
      "count           477.0  \n",
      "mean              0.0  \n",
      "std               0.0  \n",
      "min               0.0  \n",
      "25%               0.0  \n",
      "50%               0.0  \n",
      "75%               0.0  \n",
      "max               0.0  \n"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "# 1. Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Duplicates removal: Done\")\n",
    "\n",
    "# 2. Handle missing values by attack type\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "missing_values_before = df[numeric_cols].isnull().sum().sum()\n",
    "\n",
    "# Handle missing values per attack type for better representation\n",
    "for attack_type in df['attack'].unique():\n",
    "    mask = df['attack'] == attack_type\n",
    "    df.loc[mask, numeric_cols] = df.loc[mask, numeric_cols].fillna(\n",
    "        df.loc[mask, numeric_cols].mean()\n",
    "    )\n",
    "\n",
    "# Handle non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=np.number).columns\n",
    "for col in non_numeric_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "missing_values_after = df[numeric_cols].isnull().sum().sum()\n",
    "print(f\"Missing values removed: {missing_values_before - missing_values_after}\")\n",
    "\n",
    "# 3. Clean column names\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "print(\"Column names cleaned\")\n",
    "\n",
    "# 4. Handle infinite values by attack type\n",
    "infinite_values_before = df.isin([np.inf, -np.inf]).sum().sum()\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Handle infinites per attack type\n",
    "for attack_type in df['attack'].unique():\n",
    "    mask = df['attack'] == attack_type\n",
    "    df.loc[mask, numeric_cols] = df.loc[mask, numeric_cols].fillna(\n",
    "        df.loc[mask, numeric_cols].mean()\n",
    "    )\n",
    "\n",
    "infinite_values_after = df.isin([np.inf, -np.inf]).sum().sum()\n",
    "print(f\"Infinite values handled: {infinite_values_before - infinite_values_after}\")\n",
    "\n",
    "# 5. Drop single-value columns\n",
    "cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "print(f\"Dropped {len(cols_to_drop)} single-value columns\")\n",
    "\n",
    "# 6. Print statistics per attack type\n",
    "print(\"\\nFeature statistics by attack type:\")\n",
    "for attack_type in df['attack'].unique():\n",
    "    mask = df['attack'] == attack_type\n",
    "    print(f\"\\nAttack type: {attack_type}\")\n",
    "    print(f\"Number of samples: {mask.sum()}\")\n",
    "    print(\"Numeric feature statistics:\")\n",
    "    print(df.loc[mask, numeric_cols].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE:\n",
      "attack_encoded\n",
      "1    0.99987\n",
      "0    0.00013\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "attack_encoded\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Data preparation completed:\n",
      "Training set shape: (5868870, 11)\n",
      "Test set shape: (733705, 11)\n",
      "Number of features: 11\n",
      "\n",
      "Feature names:\n",
      "1. pkSeqID\n",
      "2. seq\n",
      "3. stddev\n",
      "4. N_IN_Conn_P_SrcIP\n",
      "5. min\n",
      "6. state_number\n",
      "7. mean\n",
      "8. N_IN_Conn_P_DstIP\n",
      "9. drate\n",
      "10. srate\n",
      "11. max\n"
     ]
    }
   ],
   "source": [
    "#SMOTE and Feature Preparation\n",
    "# Remove unnecessary columns\n",
    "# Modified feature preparation code\n",
    "columns_to_drop = [\n",
    "    # these are non numeric or categorical columns that are not useful for the model\n",
    "    'category', 'subcategory', 'proto', 'saddr', 'sport', 'daddr', 'dport', 'attack', 'attack_encoded',\n",
    "    \n",
    "    # Time-related columns (often not relevant for pattern detection)\n",
    "    'starttime', 'ltime',\n",
    "    \n",
    "    # Session identifiers (unique per connection)\n",
    "    'sid', 'sessionid',\n",
    "    \n",
    "    # Redundant or derived features\n",
    "    'tcprtt', 'synack', 'ackdat',  # Often correlated with other timing features\n",
    "    'state',  # Protocol state often captured by other features\n",
    "    \n",
    "    # Location or routing specific\n",
    "    'service', 'smac', 'dmac',  # MAC addresses aren't relevant for botnet detection\n",
    "    \n",
    "    # Highly sparse or constant features\n",
    "    'trans_depth', 'response_body_len'  # Often sparse in botnet traffic\n",
    "]\n",
    "\n",
    "X = df.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "y = df['attack_encoded']  # Using encoded labels for multiclass\n",
    "\n",
    "# Split data before SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Print class distribution before SMOTE\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print class distribution after SMOTE\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_resampled).value_counts(normalize=True))\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Print shapes and feature names\n",
    "print(\"\\nData preparation completed:\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
    "print(\"\\nFeature names:\")\n",
    "for i, feature in enumerate(X.columns):\n",
    "    print(f\"{i + 1}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA components initialized with multiclass support\n"
     ]
    }
   ],
   "source": [
    "#Genetic Algorithm Feature Selection\n",
    "# Clear existing DEAP creators\n",
    "if 'FitnessMax' in creator.__dict__:\n",
    "    del creator.FitnessMax\n",
    "if 'Individual' in creator.__dict__:\n",
    "    del creator.Individual\n",
    "\n",
    "# Initialize GA components\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Setup toolbox\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, \n",
    "                 toolbox.attr_bool, n=X_train_scaled.shape[1])\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define LightGBM parameters for multiclass\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(label_encoder.classes_),\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': 10,\n",
    "    'min_child_samples': 40,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.01,\n",
    "    'n_estimators': 200,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1,\n",
    "    'metric': 'multi_logloss'\n",
    "}\n",
    "\n",
    "# Define multiclass fitness function\n",
    "def evaluate_individual(individual, X, y, k=5):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0.0,\n",
    "    \n",
    "    X_selected = X[:, selected_features]\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_selected):\n",
    "        X_train_fold = X_selected[train_idx]\n",
    "        X_val_fold = X_selected[val_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**lgb_params)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate weighted metrics for multiclass\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        f1 = f1_score(y_val_fold, y_pred, average='weighted')\n",
    "        score = (accuracy + f1) / 2\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Return mean CV score with feature penalty\n",
    "    feature_penalty = 0.001 * len(selected_features)/X.shape[1]\n",
    "    return np.mean(scores) - feature_penalty,\n",
    "\n",
    "# Register GA operators\n",
    "toolbox.register(\"evaluate\", evaluate_individual, X=X_train_scaled, y=y_train_resampled)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "print(\"GA components initialized with multiclass support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA components initialized with multiclass support\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize GA components\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Setup toolbox\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, \n",
    "                 toolbox.attr_bool, n=X_train_scaled.shape[1])\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define LightGBM parameters for multiclass classification\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(label_encoder.classes_),  # Number of attack classes\n",
    "    'learning_rate': 0.01990086265614642,\n",
    "    'num_leaves': 65,\n",
    "    'max_depth': 7,\n",
    "    'min_child_samples': 40,\n",
    "    'subsample': 0.6971404971190901,\n",
    "    'colsample_bytree': 0.8912957047621014,\n",
    "    'reg_alpha': 1.1077475757773147e-05,\n",
    "    'reg_lambda': 7.505137036788418,\n",
    "    'n_estimators': 386,\n",
    "    'feature_fraction': 0.6749863286377266,\n",
    "    'bagging_fraction': 0.8742278952008886,\n",
    "    'min_child_weight': 13,\n",
    "    'metric': 'multi_logloss',  # Metric for multiclass\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Define multiclass fitness function\n",
    "def evaluate_individual(individual, X, y, k=5):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0.0,\n",
    "    \n",
    "    X_selected = X[:, selected_features]\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_selected):\n",
    "        X_train_fold = X_selected[train_idx]\n",
    "        X_val_fold = X_selected[val_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**lgb_params)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate weighted metrics for multiclass\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        precision = precision_score(y_val_fold, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_val_fold, y_pred, average='weighted')\n",
    "        \n",
    "        # Combined score with multiple metrics\n",
    "        score = (accuracy + precision + f1) / 3\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Return mean CV score with feature penalty\n",
    "    feature_penalty = 0.001 * len(selected_features)/X.shape[1]\n",
    "    return np.mean(scores) - feature_penalty,\n",
    "\n",
    "# Register GA operators\n",
    "toolbox.register(\"evaluate\", evaluate_individual, X=X_train_scaled, y=y_train_resampled)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "print(\"GA components initialized with multiclass support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Genetic Algorithm for feature selection...\n"
     ]
    }
   ],
   "source": [
    "#Run GA and Store Selected Features\n",
    "print(\"Running Genetic Algorithm for feature selection...\")\n",
    "\n",
    "# Set GA parameters for multiclass problem\n",
    "population_size = 50  # Increased for better exploration\n",
    "n_generations = 20    # More generations for complex multiclass problem\n",
    "crossover_prob = 0.8\n",
    "mutation_prob = 0.1  # Slightly increased mutation rate\n",
    "\n",
    "# Create initial population\n",
    "population = toolbox.population(n=population_size)\n",
    "\n",
    "# Statistics setup for tracking evolution\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "# Run GA with statistics\n",
    "result, logbook = algorithms.eaSimple(population, toolbox,\n",
    "                                    cxpb=crossover_prob,\n",
    "                                    mutpb=mutation_prob,\n",
    "                                    ngen=n_generations,\n",
    "                                    stats=stats,\n",
    "                                    verbose=True)\n",
    "\n",
    "# Get best solution and its features\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "selected_features = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "\n",
    "# Get feature names and their importance scores\n",
    "feature_names = X.columns\n",
    "chosen_features = list(feature_names[selected_features])\n",
    "\n",
    "# Print detailed selection results\n",
    "print(f\"\\nGA Feature Selection Results:\")\n",
    "print(f\"Number of selected features: {len(chosen_features)} out of {len(feature_names)}\")\n",
    "print(f\"Selection ratio: {len(chosen_features)/len(feature_names):.2%}\")\n",
    "\n",
    "print(\"\\nSelected Features:\")\n",
    "for i, feature in enumerate(chosen_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "# Save selected features with additional information\n",
    "feature_file = \"models_and_data/selected_features.txt\"\n",
    "os.makedirs(os.path.dirname(feature_file), exist_ok=True)\n",
    "\n",
    "with open(feature_file, \"w\") as file:\n",
    "    file.write(\"Selected Features from Genetic Algorithm\\n\")\n",
    "    file.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    file.write(f\"Total Features Selected: {len(chosen_features)} out of {len(feature_names)}\\n\")\n",
    "    file.write(f\"Selection Ratio: {len(chosen_features)/len(feature_names):.2%}\\n\\n\")\n",
    "    \n",
    "    file.write(\"GA Parameters:\\n\")\n",
    "    file.write(f\"Population Size: {population_size}\\n\")\n",
    "    file.write(f\"Number of Generations: {n_generations}\\n\")\n",
    "    file.write(f\"Crossover Probability: {crossover_prob}\\n\")\n",
    "    file.write(f\"Mutation Probability: {mutation_prob}\\n\\n\")\n",
    "    \n",
    "    file.write(\"Selected Features List:\\n\")\n",
    "    for i, feature in enumerate(chosen_features, 1):\n",
    "        file.write(f\"{i}. {feature}\\n\")\n",
    "    \n",
    "    # Add evolution statistics\n",
    "    file.write(\"\\nEvolution Statistics:\\n\")\n",
    "    for gen, stats in enumerate(logbook):\n",
    "        file.write(f\"Generation {gen}:\\n\")\n",
    "        file.write(f\"  Avg Fitness: {stats['avg']:.4f}\\n\")\n",
    "        file.write(f\"  Max Fitness: {stats['max']:.4f}\\n\")\n",
    "\n",
    "print(f\"\\nDetailed feature selection results saved to {feature_file}\")\n",
    "\n",
    "# Save selected feature indices for later use\n",
    "np.save(\"models_and_data/selected_feature_indices.npy\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM Training and Evaluation for Multimodal\n",
    "print(\"Training LightGBM with k-fold validation for multimodal data...\")\n",
    "\n",
    "# Use the features selected by GA for both modalities\n",
    "X_network = X_train_scaled[:, selected_features]\n",
    "X_host = X_train_scaled[:, selected_features]\n",
    "X_test_network = X_test_scaled[:, selected_features]\n",
    "X_test_host = X_test_scaled[:, selected_features]\n",
    "\n",
    "# Initialize metrics storage\n",
    "fold_results = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_network), 1):\n",
    "    print(f\"\\nTraining fold {fold}/5...\")\n",
    "\n",
    "    # Split data for both modalities\n",
    "    X_train_network = X_network[train_idx]\n",
    "    X_val_network = X_network[val_idx]\n",
    "    X_train_host = X_host[train_idx]\n",
    "    X_val_host = X_host[val_idx]\n",
    "    y_train_fold = y_train_resampled[train_idx]\n",
    "    y_val_fold = y_train_resampled[val_idx]\n",
    "\n",
    "    # Train network model\n",
    "    network_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    network_model.fit(X_train_network, y_train_fold)\n",
    "\n",
    "    # Train host model\n",
    "    host_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    host_model.fit(X_train_host, y_train_fold)\n",
    "\n",
    "    # Get predictions from both models\n",
    "    network_pred_proba = network_model.predict_proba(X_val_network)\n",
    "    host_pred_proba = host_model.predict_proba(X_val_host)\n",
    "\n",
    "    # Combine predictions with weighted average\n",
    "    combined_pred_proba = 0.6 * network_pred_proba + 0.4 * host_pred_proba\n",
    "    y_pred = np.argmax(combined_pred_proba, axis=1)\n",
    "\n",
    "    # Calculate metrics for multimodal prediction\n",
    "    metrics = {\n",
    "        \"Fold\": fold,\n",
    "        \"Accuracy\": accuracy_score(y_val_fold, y_pred),\n",
    "        \"Precision\": precision_score(y_val_fold, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_val_fold, y_pred, average='weighted'),\n",
    "        \"F1\": f1_score(y_val_fold, y_pred, average='weighted'),\n",
    "        \"Confusion_Matrix\": confusion_matrix(y_val_fold, y_pred)\n",
    "    }\n",
    "    fold_results.append(metrics)\n",
    "\n",
    "    # Print individual modality performance\n",
    "    network_pred = network_model.predict(X_val_network)\n",
    "    host_pred = host_model.predict(X_val_host)\n",
    "\n",
    "    print(f\"Fold {fold} Results:\")\n",
    "    print(\"Network Features:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_val_fold, network_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_val_fold, network_pred, average='weighted'):.4f}\")\n",
    "\n",
    "    print(\"\\nHost Features:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_val_fold, host_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_val_fold, host_pred, average='weighted'):.4f}\")\n",
    "\n",
    "    print(\"\\nCombined Multimodal:\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['F1']:.4f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_metrics = {\n",
    "    \"Accuracy\": np.mean([m[\"Accuracy\"] for m in fold_results]),\n",
    "    \"Precision\": np.mean([m[\"Precision\"] for m in fold_results]),\n",
    "    \"Recall\": np.mean([m[\"Recall\"] for m in fold_results]),\n",
    "    \"F1\": np.mean([m[\"F1\"] for m in fold_results])\n",
    "}\n",
    "\n",
    "print(\"\\nAverage Multimodal Metrics Across All Folds:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save the best models (by F1 score)\n",
    "best_fold = max(fold_results, key=lambda x: x['F1'])\n",
    "best_fold_idx = best_fold['Fold'] - 1\n",
    "\n",
    "# Retrain best models on full training set\n",
    "final_network_model = lgb.LGBMClassifier(**lgb_params)\n",
    "final_host_model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "final_network_model.fit(X_network, y_train_resampled)\n",
    "final_host_model.fit(X_host, y_train_resampled)\n",
    "\n",
    "# Save models and feature information\n",
    "models_dir = \"models_and_data/multimodal\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "dump(final_network_model, f\"{models_dir}/network_model.joblib\")\n",
    "dump(final_host_model, f\"{models_dir}/host_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results and Model for Multimodal Implementation\n",
    "print(\"Evaluating and saving multimodal models...\")\n",
    "\n",
    "# Evaluate both models on test set\n",
    "network_pred_proba = final_network_model.predict_proba(X_test_network)\n",
    "host_pred_proba = final_host_model.predict_proba(X_test_host)\n",
    "\n",
    "# Combine predictions with weighted average\n",
    "combined_pred_proba = 0.6 * network_pred_proba + 0.4 * host_pred_proba\n",
    "y_pred_test = np.argmax(combined_pred_proba, axis=1)\n",
    "\n",
    "# Get individual model predictions\n",
    "network_pred_test = final_network_model.predict(X_test_network)\n",
    "host_pred_test = final_host_model.predict(X_test_host)\n",
    "\n",
    "# Calculate metrics for each model type\n",
    "test_metrics = {\n",
    "    \"Network\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, network_pred_test),\n",
    "        \"Precision\": precision_score(y_test, network_pred_test, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test, network_pred_test, average='weighted'),\n",
    "        \"F1\": f1_score(y_test, network_pred_test, average='weighted'),\n",
    "        \"Confusion_Matrix\": confusion_matrix(y_test, network_pred_test)\n",
    "    },\n",
    "    \"Host\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, host_pred_test),\n",
    "        \"Precision\": precision_score(y_test, host_pred_test, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test, host_pred_test, average='weighted'),\n",
    "        \"F1\": f1_score(y_test, host_pred_test, average='weighted'),\n",
    "        \"Confusion_Matrix\": confusion_matrix(y_test, host_pred_test)\n",
    "    },\n",
    "    \"Combined\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "        \"Precision\": precision_score(y_test, y_pred_test, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test, y_pred_test, average='weighted'),\n",
    "        \"F1\": f1_score(y_test, y_pred_test, average='weighted'),\n",
    "        \"Confusion_Matrix\": confusion_matrix(y_test, y_pred_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save detailed results\n",
    "results_file = \"models_and_data/multimodal_results.txt\"\n",
    "os.makedirs(os.path.dirname(results_file), exist_ok=True)\n",
    "\n",
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\"Multimodal GA-LightGBM Results\\n\\n\")\n",
    "    \n",
    "    # Write selected features (same for both modalities in this implementation)\n",
    "    file.write(\"Selected Features Used (GA):\\n\")\n",
    "    for i, feature in enumerate(chosen_features, 1):\n",
    "        file.write(f\"{i}. {feature}\\n\")\n",
    "    \n",
    "    # Write cross-validation results\n",
    "    file.write(\"\\nCross-Validation Results:\\n\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        file.write(f\"Average {metric}: {value:.4f}\\n\")\n",
    "    \n",
    "    # Write test results for each model type\n",
    "    for model_name, metrics in test_metrics.items():\n",
    "        file.write(f\"\\n{model_name} Model Test Results:\\n\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            if metric_name != \"Confusion_Matrix\":\n",
    "                file.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "        file.write(f\"Confusion Matrix:\\n{metrics['Confusion_Matrix']}\\n\")\n",
    "    \n",
    "    # Write model parameters and weights\n",
    "    file.write(\"\\nModel Parameters:\\n\")\n",
    "    file.write(\"Network Model Weight: 0.6\\n\")\n",
    "    file.write(\"Host Model Weight: 0.4\\n\")\n",
    "\n",
    "# Save models and metadata\n",
    "models_dir = \"models_and_data/multimodal\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save final models\n",
    "dump(final_network_model, f\"{models_dir}/final_network_model.joblib\")\n",
    "dump(final_host_model, f\"{models_dir}/final_host_model.joblib\")\n",
    "\n",
    "# Save feature indices and scaler (same indices for both modalities)\n",
    "np.save(f\"{models_dir}/selected_feature_indices.npy\", selected_features)\n",
    "dump(scaler, f\"{models_dir}/feature_scaler.joblib\")\n",
    "\n",
    "print(f\"\\nResults saved to {results_file}\")\n",
    "print(f\"Models and metadata saved to {models_dir}\")\n",
    "\n",
    "# Print final performance summary\n",
    "print(\"\\nTest Set Performance Summary:\")\n",
    "for model_name, metrics in test_metrics.items():\n",
    "    print(f\"\\n{model_name} Model Performance:\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing k-fold cross-validation...\n",
      "Training on fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 232177, number of negative: 231915\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5883\n",
      "[LightGBM] [Info] Number of data points in the train set: 464092, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500282 -> initscore=0.001129\n",
      "[LightGBM] [Info] Start training from score 0.001129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 0.9955871199062263, Precision: 0.9955933998341946, Recall: 0.9955871199062263, F1: 0.9955871213918758\n",
      "Training on fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 231996, number of negative: 232097\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5881\n",
      "[LightGBM] [Info] Number of data points in the train set: 464093, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499891 -> initscore=-0.000435\n",
      "[LightGBM] [Info] Start training from score -0.000435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Accuracy: 0.9959318411004715, Precision: 0.9959331913920312, Recall: 0.9959318411004715, F1: 0.9959318343986221\n",
      "Training on fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 232242, number of negative: 231851\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5884\n",
      "[LightGBM] [Info] Number of data points in the train set: 464093, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500421 -> initscore=0.001685\n",
      "[LightGBM] [Info] Start training from score 0.001685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Accuracy: 0.995664652698172, Precision: 0.9956738418127152, Recall: 0.995664652698172, F1: 0.9956646613589956\n",
      "Training on fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 232075, number of negative: 232018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5878\n",
      "[LightGBM] [Info] Number of data points in the train set: 464093, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500061 -> initscore=0.000246\n",
      "[LightGBM] [Info] Start training from score 0.000246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Accuracy: 0.9958887461968747, Precision: 0.9958926635091048, Recall: 0.9958887461968747, F1: 0.9958887392273627\n",
      "Training on fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 231810, number of negative: 232283\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5898\n",
      "[LightGBM] [Info] Number of data points in the train set: 464093, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002038\n",
      "[LightGBM] [Info] Start training from score -0.002038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spencer Kanjera\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Accuracy: 0.9965437887315446, Precision: 0.9965480588528058, Recall: 0.9965437887315446, F1: 0.9965437589633337\n",
      "\n",
      "Results for Each Fold:\n",
      "Fold 1:\n",
      "  Accuracy: 0.9955871199062263\n",
      "  Precision: 0.9955933998341946\n",
      "  Recall: 0.9955871199062263\n",
      "  F1 Score: 0.9955871213918758\n",
      "  Confusion Matrix:\n",
      "[[57767   359]\n",
      " [  153 57745]]\n",
      "\n",
      "Fold 2:\n",
      "  Accuracy: 0.9959318411004715\n",
      "  Precision: 0.9959331913920312\n",
      "  Recall: 0.9959318411004715\n",
      "  F1 Score: 0.9959318343986221\n",
      "  Confusion Matrix:\n",
      "[[57660   284]\n",
      " [  188 57891]]\n",
      "\n",
      "Fold 3:\n",
      "  Accuracy: 0.995664652698172\n",
      "  Precision: 0.9956738418127152\n",
      "  Recall: 0.995664652698172\n",
      "  F1 Score: 0.9956646613589956\n",
      "  Confusion Matrix:\n",
      "[[57814   376]\n",
      " [  127 57706]]\n",
      "\n",
      "Fold 4:\n",
      "  Accuracy: 0.9958887461968747\n",
      "  Precision: 0.9958926635091048\n",
      "  Recall: 0.9958887461968747\n",
      "  F1 Score: 0.9958887392273627\n",
      "  Confusion Matrix:\n",
      "[[57703   320]\n",
      " [  157 57843]]\n",
      "\n",
      "Fold 5:\n",
      "  Accuracy: 0.9965437887315446\n",
      "  Precision: 0.9965480588528058\n",
      "  Recall: 0.9965437887315446\n",
      "  F1 Score: 0.9965437589633337\n",
      "  Confusion Matrix:\n",
      "[[57472   286]\n",
      " [  115 58150]]\n",
      "\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Accuracy: 0.9959232297266579\n",
      "Precision: 0.9959282310801703\n",
      "Recall: 0.9959232297266579\n",
      "F1 Score: 0.9959232230680379\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Save the average confusion matrix to a text file\u001b[39;00m\n\u001b[0;32m     93\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels_and_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 94\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(output_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     96\u001b[0m results_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_validation_results.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Multimodal k-fold cross-validation using GA-selected features\n",
    "\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "conf_matrices = []\n",
    "\n",
    "print(\"Performing multimodal k-fold cross-validation...\")\n",
    "fold = 1\n",
    "fold_results = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, val_index in kf.split(X_network):\n",
    "    print(f\"Training on fold {fold}...\")\n",
    "\n",
    "    # Split the data for both modalities (same features in this implementation)\n",
    "    X_train_network, X_val_network = X_network[train_index], X_network[val_index]\n",
    "    X_train_host, X_val_host = X_host[train_index], X_host[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_resampled[train_index], y_train_resampled[val_index]\n",
    "\n",
    "    # Train models for each modality\n",
    "    network_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    host_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    network_model.fit(X_train_network, y_train_fold)\n",
    "    host_model.fit(X_train_host, y_train_fold)\n",
    "\n",
    "    # Predict probabilities and combine\n",
    "    network_pred_proba = network_model.predict_proba(X_val_network)\n",
    "    host_pred_proba = host_model.predict_proba(X_val_host)\n",
    "    combined_pred_proba = 0.6 * network_pred_proba + 0.4 * host_pred_proba\n",
    "    y_pred = np.argmax(combined_pred_proba, axis=1)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "    precision = precision_score(y_val_fold, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val_fold, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val_fold, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred)\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "\n",
    "    fold_results.append({\n",
    "        \"Fold\": fold,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Confusion Matrix\": conf_matrix\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_conf_matrix = np.sum(conf_matrices, axis=0)\n",
    "\n",
    "print(\"\\nResults for Each Fold:\")\n",
    "for result in fold_results:\n",
    "    print(f\"Fold {result['Fold']}:\")\n",
    "    print(f\"  Accuracy: {result['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {result['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {result['Recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {result['F1 Score']:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{result['Confusion Matrix']}\\n\")\n",
    "\n",
    "print(\"\\nAverage Metrics Across All Folds:\")\n",
    "print(f\"Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f}\")\n",
    "print(f\"Recall: {avg_recall:.4f}\")\n",
    "print(f\"F1 Score: {avg_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(avg_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics saved to models_and_data/performance_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "output_file = \"models_and_data/performance_metrics.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Performance Metrics:\\n\")\n",
    "    file.write(f\"Average Accuracy: {avg_accuracy}\\n\")\n",
    "    file.write(f\"Average Precision: {avg_precision}\\n\")\n",
    "    file.write(f\"Average Recall: {avg_recall}\\n\")\n",
    "    file.write(f\"Average F1 Score: {avg_f1}\\n\")\n",
    "    file.write(\"\\nConfusion Matrix (Summed Across Folds):\\n\")\n",
    "    file.write(f\"{avg_conf_matrix}\\n\")\n",
    "    file.write(\"\\nResults for Each Fold:\\n\")\n",
    "    for result in fold_results:\n",
    "        file.write(f\"Fold {result['Fold']}:\\n\")\n",
    "        file.write(f\"  Accuracy: {result['Accuracy']}\\n\")\n",
    "        file.write(f\"  Precision: {result['Precision']}\\n\")\n",
    "        file.write(f\"  Recall: {result['Recall']}\\n\")\n",
    "        file.write(f\"  F1 Score: {result['F1 Score']}\\n\")\n",
    "        file.write(f\"  Confusion Matrix:\\n{result['Confusion Matrix']}\\n\\n\")\n",
    "\n",
    "print(f\"Performance metrics saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_and_data/final_model.joblib\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save the trained model\n",
    "model_file = \"models_and_data/final_model.joblib\"\n",
    "dump(final_model, model_file)\n",
    "\n",
    "print(f\"Model saved to {model_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
