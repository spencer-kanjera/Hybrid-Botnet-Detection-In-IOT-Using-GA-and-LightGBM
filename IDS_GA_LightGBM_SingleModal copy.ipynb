{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the needed packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from deap import base, creator, tools, algorithms\n",
    "import lightgbm as lgb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monday = pd.read_csv(\"C:/VS code projects/data_files/Monday-WorkingHours.pcap_ISCX.csv\")\n",
    "df_tuesday = pd.read_csv(\"C:/VS code projects/data_files/Tuesday-WorkingHours.pcap_ISCX.csv\")\n",
    "df_wednesday = pd.read_csv(\"C:/VS code projects/data_files/Wednesday-workingHours.pcap_ISCX.csv\")\n",
    "df_thursday_afternoon = pd.read_csv(\"C:/VS code projects/data_files/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\n",
    "df_thursday_morning= pd.read_csv(\"C:/VS code projects/data_files/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
    "df_friday_afternoon_ddos = pd.read_csv(\"C:/VS code projects/data_files/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
    "df_friday_afternoon_portscan = pd.read_csv(\"C:/VS code projects/data_files/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\n",
    "df_friday_morning = pd.read_csv(\"C:/VS code projects/data_files/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n",
    "df_monday.columns = df_monday.columns.str.strip()\n",
    "df_tuesday.columns = df_tuesday.columns.str.strip()\n",
    "df_wednesday.columns = df_wednesday.columns.str.strip()\n",
    "df_thursday_afternoon.columns = df_thursday_afternoon.columns.str.strip()\n",
    "df_thursday_morning.columns = df_thursday_morning.columns.str.strip()\n",
    "df_friday_afternoon_ddos.columns = df_friday_afternoon_ddos.columns.str.strip()\n",
    "df_friday_afternoon_portscan.columns = df_friday_afternoon_portscan.columns.str.strip()\n",
    "df_friday_morning.columns = df_friday_morning.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spencer Kanjera\\AppData\\Local\\Temp\\ipykernel_25876\\2654131954.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Label', group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42, ignore_index=True))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    557646\n",
       "1    557646\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_monday,df_tuesday,df_wednesday, df_thursday_afternoon, df_thursday_morning, df_friday_afternoon_ddos, df_friday_afternoon_portscan, df_friday_morning], ignore_index=True)\n",
    "df['Label'] = df['Label'].apply(lambda x: 1 if x == 'BENIGN' else 0)\n",
    "\n",
    "# Find the minimum class count\n",
    "min_count = df['Label'].value_counts().min()\n",
    "\n",
    "\n",
    "# Perform undersampling to balance the dataset\n",
    "df = df.groupby('Label', group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42, ignore_index=True))\n",
    "\n",
    "# Check the class distribution\n",
    "df['Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n",
      "Done preprocessing dataset\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing dataset\n",
    "print(\"Preprocessing dataset...\")\n",
    "\n",
    "# Step 1: Replace infinite values with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Step 2: Handle NaN values (e.g., fill with 0 or column mean)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "print(\"Done preprocessing dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    557646\n",
       "1    557646\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Encode categorical labels\n",
    "#label_encoder = LabelEncoder()\n",
    "\n",
    "#df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "#print(\"Labels encoded successfully!\")\n",
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Separate features and labels\n",
    "X = df.drop('Label', axis=1)\n",
    "Y = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Normalize numerical features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating feature importance using correlation...\n",
      "Features with correlation above threshold: 23\n",
      "0: Destination Port\n",
      "1: Flow Duration\n",
      "2: Bwd Packet Length Max\n",
      "3: Bwd Packet Length Min\n",
      "4: Bwd Packet Length Mean\n",
      "5: Bwd Packet Length Std\n",
      "6: Flow IAT Std\n",
      "7: Flow IAT Max\n",
      "8: Fwd IAT Total\n",
      "9: Fwd IAT Std\n",
      "10: Fwd IAT Max\n",
      "11: Min Packet Length\n",
      "12: Max Packet Length\n",
      "13: Packet Length Mean\n",
      "14: Packet Length Std\n",
      "15: Packet Length Variance\n",
      "16: PSH Flag Count\n",
      "17: URG Flag Count\n",
      "18: Average Packet Size\n",
      "19: Avg Bwd Segment Size\n",
      "20: Idle Mean\n",
      "21: Idle Max\n",
      "22: Idle Min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter Method: Calculate correlations between features and labels\n",
    "print(\"Calculating feature importance using correlation...\")\n",
    "correlations = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    if np.std(X_train[:, i]) == 0:  # Skip constant features\n",
    "        correlations.append(0)\n",
    "    else:\n",
    "        correlations.append(abs(pearsonr(X_train[:, i], y_train)[0]))\n",
    "correlation_threshold = 0.2  # Define a threshold to filter irrelevant features\n",
    "relevant_features = [i for i, corr in enumerate(correlations) if corr > correlation_threshold]\n",
    "print(f\"Features with correlation above threshold: {len(relevant_features)}\")\n",
    "\n",
    "# Subset the data with relevant features only\n",
    "X_train = X_train[:, relevant_features]\n",
    "X_test = X_test[:, relevant_features]\n",
    "feature_names = [df.columns[i] for i in relevant_features]\n",
    "[print(f\"{i}: {feature_names[i]}\") for i in range(len(feature_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Genetic Algorithm for feature selection...\n"
     ]
    }
   ],
   "source": [
    "# GA Feature Selection\n",
    "print(\"Defining Genetic Algorithm for feature selection...\")\n",
    "\n",
    "# Define GA components\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X_train.shape[1])\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fitness function\n",
    "def fitness(individual, X_train, y_train):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:  # Avoid empty feature sets\n",
    "        return 0,\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    model = lgb.LGBMClassifier(random_state=42)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    accuracy = model.score(X_train_selected, y_train)  # Training accuracy\n",
    "    return accuracy,\n",
    "\n",
    "toolbox.register(\"evaluate\", fitness, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GA parameters\n",
    "# Define GA parameters\n",
    "# population: The initial population size (number of individuals)\n",
    "# ngen: Number of generations to evolve\n",
    "# cxpb: Crossover probability (probability of mating two individuals)\n",
    "# mutpb: Mutation probability (probability of mutating an individual)\n",
    "\n",
    "population = toolbox.population(n=100)  # Increased population size for better diversity\n",
    "ngen = 50  # Increased number of generations for more thorough exploration\n",
    "cxpb = 0.8  # Higher crossover probability to encourage exploration\n",
    "mutpb = 0.1  # Lower mutation probability to reduce randomness\n",
    "\n",
    "# Run the GA\n",
    "result_population = algorithms.eaSimple(population, toolbox, cxpb=cxpb, mutpb=mutpb, ngen=ngen, verbose=False)\n",
    "best_individual = tools.selBest(result_population[0], k=1)[0]\n",
    "selected_features = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "\n",
    "# Print the chosen features\n",
    "chosen_features = [feature_names[i] for i in selected_features]\n",
    "print(\"Selected Features:\")\n",
    "print(chosen_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model with selected features...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train a model on the selected features\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining final model with selected features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m X_train_selected \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m[:, selected_features]\n\u001b[0;32m      4\u001b[0m X_test_selected \u001b[38;5;241m=\u001b[39m X_test[:, selected_features]\n\u001b[0;32m      5\u001b[0m X_train_selected \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train_selected, columns\u001b[38;5;241m=\u001b[39m[feature_names[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m selected_features])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Train a model on the selected features\n",
    "print(\"Training final model with selected features...\")\n",
    "X_train_selected = X_train[:, selected_features]\n",
    "X_test_selected = X_test[:, selected_features]\n",
    "X_train_selected = pd.DataFrame(X_train_selected, columns=[feature_names[i] for i in selected_features])\n",
    "X_test_selected = pd.DataFrame(X_test_selected, columns=[feature_names[i] for i in selected_features])\n",
    "\n",
    "final_model = lgb.LGBMClassifier(\n",
    "    random_state=42,  # Ensures reproducibility by setting a fixed random seed\n",
    "    learning_rate=0.10078188554336627,  # Controls the step size during optimization\n",
    "    num_leaves=62,  # Maximum number of leaves in one tree\n",
    "    max_depth=12,  # Maximum depth of a tree to prevent overfitting\n",
    "    n_estimators=289,  # Number of boosting iterations (trees)\n",
    "    boosting_type='gbdt',  # Gradient Boosting Decision Tree (default boosting type)\n",
    "    class_weight='balanced',  # Automatically adjusts weights inversely proportional to class frequencies\n",
    "    verbose_eval=1,  # Controls verbosity of the training process\n",
    "    min_child_samples=94,  # Minimum number of data points in a leaf node\n",
    "    subsample=0.7102007989557683,  # Fraction of samples used for training each tree\n",
    "    colsample_bytree=0.5061939258896568,  # Fraction of features used for training each tree\n",
    "    reg_alpha=1.9225920516937184e-7,  # L1 regularization term to prevent overfitting\n",
    "    reg_lambda=4.6875142019482343e-7  # L2 regularization term to prevent overfitting\n",
    ")\n",
    "final_model.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = final_model.predict(X_test_selected)\n",
    "\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = final_model.feature_importances_\n",
    "\n",
    "# Combine feature names and their importance into a DataFrame for better readability\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_selected.columns,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "                   Feature  Importance\n",
      "0         Destination Port        1913\n",
      "4             Flow IAT Max        1147\n",
      "5            Fwd IAT Total        1101\n",
      "1   Bwd Packet Length Mean         840\n",
      "3             Flow IAT Std         777\n",
      "9   Packet Length Variance         537\n",
      "6              Fwd IAT Std         523\n",
      "8       Packet Length Mean         510\n",
      "2    Bwd Packet Length Std         505\n",
      "7              Fwd IAT Max         482\n",
      "12     Average Packet Size         445\n",
      "10          PSH Flag Count         430\n",
      "11          URG Flag Count         246\n",
      "14                Idle Max         205\n",
      "15                Idle Min         139\n",
      "13    Avg Bwd Segment Size           0\n"
     ]
    }
   ],
   "source": [
    "# Print the feature importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy: 0.9958\n",
      "Precision: 0.9958\n",
      "Recall: 0.9958\n",
      "F1 Score: 0.9958\n",
      "Confusion Matrix:\n",
      "[[166484    740]\n",
      " [   661 166703]]\n"
     ]
    }
   ],
   "source": [
    "# Print evaluation metrics\n",
    "print(f\"Final Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    167224\n",
      "           1       1.00      1.00      1.00    167364\n",
      "\n",
      "    accuracy                           1.00    334588\n",
      "   macro avg       1.00      1.00      1.00    334588\n",
      "weighted avg       1.00      1.00      1.00    334588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
